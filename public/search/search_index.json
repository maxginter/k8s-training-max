{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Kubernetes in Action - Guide Before your Start Install XXX on https://www.takima.fr Change your configuration in thisfile and thisfile Day 1 Handson session Day one steps are available here","title":"Briefing"},{"location":"#kubernetes-in-action-guide","text":"","title":"Kubernetes in Action - Guide"},{"location":"#before-your-start","text":"Install XXX on https://www.takima.fr Change your configuration in thisfile and thisfile Day 1 Handson session Day one steps are available here","title":"Before your Start"},{"location":"about/","text":"About Us The Dev Team Aur\u00e9lien, Benjamin and Lo\u00efc are working on different topics on a daily basis, although they all like driving themselves beyond their comfort zone, especially around DevOps topics. Most developers are good at programming, and we get better at doing large-scale architectures every day. Thanks to technology, we are adding layers of abstraction that allows us to develop faster, and complex architectures require new conception habits. We wanted through this workshop to reconnect you with this world, by exploring the latest Ops and DevOps trends and practices. The Sponsors Takima has been supporting us and gave us company time to work on this project. They also provided support and coaching for the content itself and speaker-preparation. It is an awesome company, you should look it up :)","title":"About"},{"location":"about/#about-us","text":"","title":"About Us"},{"location":"about/#the-dev-team","text":"Aur\u00e9lien, Benjamin and Lo\u00efc are working on different topics on a daily basis, although they all like driving themselves beyond their comfort zone, especially around DevOps topics. Most developers are good at programming, and we get better at doing large-scale architectures every day. Thanks to technology, we are adding layers of abstraction that allows us to develop faster, and complex architectures require new conception habits. We wanted through this workshop to reconnect you with this world, by exploring the latest Ops and DevOps trends and practices.","title":"The Dev Team"},{"location":"about/#the-sponsors","text":"Takima has been supporting us and gave us company time to work on this project. They also provided support and coaching for the content itself and speaker-preparation. It is an awesome company, you should look it up :)","title":"The Sponsors"},{"location":"ch1-discover-kubernetes/","text":"Day 1: Discover Kubernetes Initialisation des outils Vous avez d\u00fb recevoir un petit cadeau dans votre bo\u00eete mail nomm\u00e9 kubeconfig. Ce fichier est la clef d\u2019acc\u00e8s au cluster Kubernetes qui a \u00e9t\u00e9 sp\u00e9cialement provisionn\u00e9 pour que vous puissiez vous amuser \u00e0 d\u00e9ployer des ressources, publier des services et les scaler. Pour pouvoir l\u2019utiliser, vous devrez d\u2019abord installer kubectl. Il s\u2019agit de l'outil d\u2019administration en ligne de commande (CLI) pour manager Kubernetes. Installation d'un \u00e9diteur de code Note Il est conseill\u00e9 d'installer un \u00e9diteur de code complet pour l'\u00e9dition des fichiers tout au long de cette formation. Si vous n'en avez pas, vous pouvez utiliser VsCode . Installation de kubectl Linux / MacOS T\u00e9l\u00e9chargez le client : curl -LO https://storage.googleapis.com/kubernetes-release/release/ $( curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt ) /bin/linux/amd64/kubectl Le rendre ex\u00e9cutable : chmod +x ./kubectl D\u00e9placez le binaire dans votre PATH : sudo mv ./kubectl /usr/local/bin/kubectl Testez pour vous assurer que la version que vous avez install\u00e9e est \u00e0 jour : kubectl version --client Windows T\u00e9l\u00e9chargez le client : curl -LO https://storage.googleapis.com/kubernetes-release/release/v1.23.0/bin/windows/amd64/kubectl.exe Ajoutez l'ex\u00e9cutable dans le PATH. Tutoriel sous Windows 10 Testez pour vous assurer que la version que vous avez install\u00e9e est \u00e0 jour : kubectl version --client Kubeconfig Votre client Kubectl est maintenant op\u00e9rationnel. Pour administrer un cluster Kubernetes, vous allez devoir utiliser les informations de connexion pr\u00e9sentes dans le fichier kubeconfig. Pour cela, vous devez simplement placer ce fichier dans $HOME/.kube/config . Question Quelles sont les informations que l'on retrouve dans ce fichier ? Lens Lens est un IDE (Integrated Development Environment) qui permet d'administrer localement plusieurs clusters kubernetes : Installez l\u2019IDE : https://k8slens.dev/ (installez la version correspondante \u00e0 votre OS) Ouvrez l\u2019IDE : le login est inutile et vous pouvez directement cliquer sur \"Browse catalog\". Vous devriez y trouver votre .kube/config. Premi\u00e8res commandes Super ! Vous \u00eates maintenant pr\u00eat \u00e0 utiliser Kubernetes. Comme tout bon outil d\u2019administration, kubectl propose un helper assez fourni. Vous pouvez y jeter un coup d'\u0153il : kubectl -h Pour commencer, il faut savoir que Kubernetes fonctionne avec des Namespaces pour s\u00e9parer de mani\u00e8re logique les ressources. Cela permet de s\u00e9parer les projets ou les environnements par exemple et de d\u00e9finir des droits utilisateur sur chacun d\u2019entre eux. Dans le cadre de cet atelier, vous aurez acc\u00e8s \u00e0 un Namespace personnel qui a pour libell\u00e9 votre username : si votre mail est toto@exemple.com, votre namespace se nommera toto. Maintenant, essayez de conna\u00eetre les pods qui tournent dans votre namespace : kubectl get pods -n votre_namespace Maintenant, essayez la m\u00eame commande sur un autre namespace : kubectl get pods -n default Question Quelle est la diff\u00e9rence ? Etape 1 : Premi\u00e8res ressources : Pods/Replicaset/Deployment Pods Dans le monde Kubernetes, TOUT est ressource : un d\u00e9ploiement est une ressource, un pod est une ressource, une configuration est une ressource, un disque est une ressource, etc\u2026 Tous ces objets Kubernetes peuvent \u00eatre d\u00e9crits dans un fichier (souvent un fichier yaml). Le Pod est la ressource minimale et triviale qu\u2019il est possible de d\u00e9ployer dans Kubernetes en terme de ressources applicatives. Pour faire simple, un pod est une ressource qui h\u00e9berge un ou plusieurs containers. Comme pour un container d\u00e9marr\u00e9 avec Docker, vous devez d\u00e9finir une image Docker \u00e0 consommer, des variables, des volumes. Par exemple : kubectl run mynginx --image nginx Pour chaque ressource, nous proposerons un lien qui renvoit vers la documentation officielle de Kubernetes, qui est une source fiable et souvent utile pour se rensigner sur les diff\u00e9rentes caract\u00e9ristiques des ressources. Pour le Pod, c'est par ici : https://kubernetes.io/docs/concepts/workloads/pods/ Info Derri\u00e8re cette commande, un fichier yaml de type POD est g\u00e9n\u00e9r\u00e9 et est envoy\u00e9 au KubeApi pour la cr\u00e9ation. Observez la cr\u00e9ation du pod : kubectl get pods mynginx Vous pouvez aussi d\u00e9crire le fichier yaml qui d\u00e9fini le pod : kubectl get pods mynginx -o yaml Question Quelles sont les propri\u00e9t\u00e9s principales que l'on retrouve ? Vous retrouvez dans les root properties : apiVersion kind metadata spec Maintenant, d\u00e9truisez le pod : kubectl delete pods nom_du_pod Voila vous avez cr\u00e9\u00e9 et supprim\u00e9 votre premi\u00e8re ressource dans kubernetes. Ce n'est pas si compliqu\u00e9 n'est ce pas ? Pod Yaml Definition Nous allons imaginer une application vide, qui s'appelle Unicorn. Pour l'instant, Unicorn n'est qu'un container Nginx tout vide, c'est un front . Par soucis de convention, nous vous proposons de cr\u00e9er un fichier unicorn-front-pod.yaml Note Comme dans kubernetes TOUT est ressource yaml nous te conseillons de cr\u00e9er un dossier et une arborescence pour pouvoir les \u00e9diter et les modifier. par exemple k8s-TP1/step-1 dans lequel vous placerez vos ressources yaml manipul\u00e9es. Pour appliquer une ressource kubernetes d\u00e9crite en tant que yaml la commande est toujours la m\u00eame : kubectl apply -f monfichier.yaml A vous de jouer. Essayez de cr\u00e9er la m\u00eame ressource avec un fichier yaml : Tip Pour avoir le yaml correspondant \u00e0 une ressource \u00e0 cr\u00e9er, vous pouvez utiliser la fonction dry-run, exemple : kubectl run mynginx2 --image nginx --dry-run = client -o yaml > unicorn-front-pod.yml #cette commande ne cr\u00e9era pas le pod dans kubernetes mais dumpera l'\u00e9quivalant du fichier yaml qui aurait \u00e9t\u00e9 envoy\u00e9 au KubeApi pour la cr\u00e9ation. c'est tr\u00e8s utiles pour avoir un template de ressource voulu et ne pas d\u00e9marrer avec un yaml vierge # Pour appliquer une ressource d\u00e9crite dans un yaml : kubectl apply -f unicorn-front-pod.yml Comme avec docker, dans kubernetes, vous pouvez acc\u00e9der \u00e0 votre container en mode ex\u00e9cution (remarquez que l\u2019on retrouve des commandes Docker) : kubectl exec nom_du_pod -it -- bash # -it pour interactive : permet d'avoir la main dans l'invite de commande Ou voir les logs : kubectl logs nom_du_pod D\u00e9truisez de nouveau le pod : kubectl delete pods nom_du_pod Info Attention, le pod ne s'appelle plus \u201cmynginx\u201d, v\u00e9rifiez le fichier .yaml g\u00e9n\u00e9r\u00e9 pr\u00e9c\u00e9demment. Replicaset Ok, c\u2019est bien : vous avez r\u00e9ussi \u00e0 d\u00e9ployer un container dans un Pod ! Mais qu'en est-il de la haute disponibilit\u00e9 ou de la scalabilit\u00e9 promise ? Pour scaler un applicatif, il faut tout simplement rajouter des Pods du m\u00eame applicatif. Vous pourriez cr\u00e9er \u00e0 la main plusieurs Pods avec la m\u00eame image mais cela ne serait pas tr\u00e8s pratique. Il est beaucoup plus efficace de les regrouper au sein d\u2019un groupe de ressources. C\u2019est dans cet objectif que Kubernetes d\u00e9finit les ReplicaSet permettant de contr\u00f4ler plusieurs pods d\u2019un m\u00eame applicatif (m\u00eame image Docker) partageant les m\u00eames configs, les m\u00eames propri\u00e9t\u00e9s. Voici un exemple de ressource ReplicaSet : apiVersion : apps/v1 kind : ReplicaSet metadata : name : unicorn-front-replicaset labels : app : unicorn-front spec : template : metadata : name : unicorn-front-pod labels : app : unicorn-front spec : containers : - name : unicorn-front image : nginx replicas : 3 selector : matchLabels : app : unicorn-front Question Que remarquez-vous dans la description des properties spec: template ? \u00c0 quoi sert le selector: matchLabels ? Editez le yaml unicorn-front-replicaset.yaml et d\u00e9ployez ce ReplicaSet sur votre cluster avec kubectl apply . Question Combien y a-t'il de pods d\u00e9ploy\u00e9s dans votre namespace ? Maintenant, supprimez un Pod . Question Que se passe-t'il ? Supprimez le ReplicaSet . Question Que se passe-t'il ? Deployment Une ressource Kubernetes de type Deployment permet de facilement piloter des ReplicaSet`. En pratique, c'est typiquement avec cette ressource que sont d\u00e9ploy\u00e9es les applications. En effet, le Deployment g\u00e8re la notion de cycle de vie des Pods (via les version de ReplicaSet pilot\u00e9s), notamment avec la d\u00e9finition de RollingUpgrade lors d\u2019un d\u00e9ploiement de nouvelles versions. Cependant, il faut que la strat\u00e9gie par d\u00e9faut de RollingUpdate soit conserv\u00e9e ( .spec.strategy.type ) : cela facilite les rollbacks vers une version ant\u00e9rieure. Lors de la modification de la version d\u2019une image Docker consomm\u00e9e dans un d\u00e9ploiement, un nouveau ReplicaSet sera cr\u00e9\u00e9. L\u2019ancien ReplicaSet correspondant \u00e0 l'ancienne version de l\u2019image reste et les Pods sont vid\u00e9s de l\u2019ancien ReplicaSet vers le nouveau ( RollingUpgrade ). Ce transfert se fait au \u201cfil de l'eau\u201d, de mani\u00e8re \u00e0 ce qu\u2019il n\u2019y ait pas d\u2019interruption de service (par d\u00e9faut Kubernetes tente de garder 75% des pods actifs). Dans le cas du TP, il ne d\u00e9truit les Pods obsol\u00e8tes qu\u2019apr\u00e8s que le nouveau Pod obtienne le statut Running. De cette mani\u00e8re, il est possible de facilement Rollback (processus inverse, les Pods allant du nouveau ReplicaSet vers l\u2019ancien). La cr\u00e9ation d\u2019un Deployment entra\u00eene la cr\u00e9ation d\u2019un ReplicaSet et donc la cr\u00e9ation d\u2019un ou plusieurs Pods. La ressource Deployment est tr\u00e8s proche de celle d\u2019un ReplicaSet : apiVersion : apps/v1 kind : Deployment metadata : name : unicorn-front-deployment labels : app : unicorn-front spec : replicas : 3 selector : matchLabels : app : unicorn-front template : metadata : labels : app : unicorn-front spec : containers : - name : unicorn-front image : nginx:1.7.9 ports : - containerPort : 80 Question Quels sont les changements par rapport au ReplicaSet ? Editer un yaml unicorn-front-deployment.yaml et d\u00e9ployez ce Deployment . Question Combien y a-t'il de ReplicaSet ? De Pods ? Commandes utiles : kubectl get all kubectl rollout status deployment.v1.apps/unicorn-front-deployment Maintenant, la notion d\u2019upgrade : c\u2019est en pratique ce qu\u2019il se passe lorsque l\u2019on r\u00e9alise un d\u00e9ploiement applicatif d\u2019une nouvelle release (via un Continuous Deployment par exemple). Pour cela, changez la version de l\u2019image nginx : Modifier une image docker dans un deployment revient a modifier cette ressource deployment. Pour modifier une ressource, vous avez 3 solutions : kubectl set ou kubectl patch kubectl set image deployment/unicorn-front-deployment nginx = nginx:1.9.1 --record Editer le fichier yaml en local et faire un kubectl apply -f monfichier.yaml Il est possible \u00e9galement d'\u00e9diter le yaml d\u00e9finition de d\u00e9ploiement en changeant l\u2019image et d'utiliser cette commande : kubectl apply -f ... Editer directement la ressouce avec kubectl edit kubectl edit deployment.v1.apps/unicorn-front-deployment Info On preferera r\u00e9aliser des updates de ressources en \u00e9ditant son fichier yaml (ici unicorn-front-deployment.yaml) en utilisant donc la 2\u00e8me solution. Cela permet d'avoir la bonne version dans son dossier (fichier synchronis\u00e9 avec ce qui est sur le cluster). Regardez le statut du RollingUpgrade : kubectl rollout status deployment.v1.apps/unicorn-front-deployment Si vous utilisez la commande rapidement, vous verrez le RollingUpgrade en cours. Question Une fois termin\u00e9, combien y a-t-il de replicaset ? Combien y a-t-il de Pods ? Allez voir les logs des \u00e9v\u00e9nements du d\u00e9ploiement avec kubectl describe deployments . Qu\u2019observez vous ? Commande utile : kubectl describe deployments Faire un Rollback Mettez \u00e0 jour votre d\u00e9ploiement avec une nouvelle version de l\u2019image nginx. kubectl set image deployment.v1.apps/unicorn-front-deployment nginx = nginx:1.91-falseimage --record = true Observez votre Deployment , vos Pods et vos ReplicaSets . Question Que se passe-t'il ? Pourquoi ? Dans ce cas de figure, l'objectif est de revenir \u00e0 une version stable de Deployment (une version fonctionnelle avec une bonne image nginx). Pour commencer, v\u00e9rifiez les r\u00e9visions de ce d\u00e9ploiement : kubectl rollout history deployment.v1.apps/unicorn-front-deployment Question Combien y a-t-il de r\u00e9visions ? \u00c0 quoi correspond le champ CHANGE-CAUSE ? Vous pouvez afficher les d\u00e9tails de chaque r\u00e9vision (pour la r\u00e9vision 2 par exemple) avec cette commande : kubectl rollout history deployment.v1.apps/unicorn-front-deployment --revision = 2 Maintenant que la version stable est identifi\u00e9e, voici la commande pour retourner \u00e0 cette version : kubectl rollout undo deployment.v1.apps/unicorn-front-deployment --to-revision = 2 Info Pour revenir \u00e0 la version pr\u00e9c\u00e9dente, il est aussi possible d'utiliser cette commade : kubectl rollout undo deployment.v1.apps/unicorn-front-deployment Mettre \u00e0 l'\u00e9chelle Le d\u00e9ploiement de nouvelles versions (releases) est maintenant ma\u00eetris\u00e9 mais dans l'hypoth\u00e8se que le site internet est promu au journal t\u00e9l\u00e9vis\u00e9 et qu\u2019un afflux cons\u00e9quent de visiteurs se connectent dessus, que faire ? Il faut tout simplement d\u00e9ployer plus de Pods de l'applicatif. C\u2019est ce qui est appell\u00e9 scaler une application. Scalez le serveur nginx \u00e0 5 : kubectl scale deployment.v1.apps/unicorn-front-deployment --replicas = 5 Info Encore une fois cette commande est pratique pour le cadre du TP mais en pratique on pref\u00e9rera \u00e9diter son fichier yaml de d\u00e9ploiment et mettre replica: 5 puis faire un kubectl apply -f unicorn-front-deployment.yaml Question Combien y a-t'il de Pods ? Info Pour aller plus loin, Kubernetes impl\u00e9mente, si c\u2019est activ\u00e9, un syst\u00e8me d\u2019autoscaling appel\u00e9 HPA ( Horizontal Pods Autoscaling ). Kubernetes se chargera de scaler automatiquement l'application en fonction de trigger d\u00e9finis par exemple sur la consommation CPU ou la RAM. Si vous \u00eates rapide une impl\u00e9mentation du HPA est propos\u00e9 en bonus. Mettre en standby un deploiement Mettez en pause un Deployment : kubectl rollout pause deployment.v1.apps/unicorn-front-deployment kubectl set image deployment.v1.apps/unicorn-front-deployment nginx = nginx:1.20.2 Question Que se passe-t-il au niveau ReplicaSet ? kubectl rollout resume deployment.v1.apps/unicorn-front-deployment Question Que se passe-t-il au niveau ReplicaSet ? Info La pause permet de modifier le d\u00e9ploiement (encore une autre version, des properties du Deployment comme les limits/ressources) sans d\u00e9clencher de RollingUpgrade . Bonus Lorsque des applications sont d\u00e9ploy\u00e9es sous forme de containers (dans des Pods), il est indispensable de contr\u00f4ler les ressources qu\u2019elles consomment. Imaginez qu\u2019un Pod se mette \u00e0 consommer plusieurs dizaines de gigabytes de m\u00e9moire vive, quel serait l\u2019impact sur les autres Pods tournant sur le m\u00eame Worker Node (m\u00eame serveur) ? Pour emp\u00eacher cela, Kubernetes impl\u00e9mente la notion de Ressources Management. Example resources : requests : memory : \"64Mi\" cpu : \"250m\" limits : memory : \"128Mi\" cpu : \"500m\" Ce block se place au niveau spec.container * Le requests est la ressource monopolis\u00e9e au start du Container. * La limits est le maximum de m\u00e9moire et CPU consommable par le Pod. Impl\u00e9mentez une ressource management pour votre Deployment avec ces sp\u00e9cifications : * Requests : m\u00e9moire : 32 Mo / CPU : 100m * Limits : m\u00e9moire 256 Mo / CPU : 400m Check Vous devez avoir les fichiers suivants dans le dossier step-1 : . \u251c\u2500\u2500 unicorn-front-deployment.yaml \u251c\u2500\u2500 unicorn-front-replicaset.yaml \u2514\u2500\u2500 unicorn-front-pod.yaml Etape 2 : Publication Service/Ingress Note Vous pouvez maintenant vous placer dans le dossier k8s-TP1/step-2 dans lequel vous placerez vos ressources yaml manipul\u00e9es pendant cette \u00e9tape. Gardez les ressources yaml du step-1 qui pourront vous servir de r\u00e9f\u00e9rence. La plupart du temps l'applicatif qui tourne dans un Pod Kubernetes doit d\u00e9livrer un service (que ce soit une Api, un Front ou une base de donn\u00e9es, par exemple) et doit donc \u00eatre requ\u00eat\u00e9. Kubernetes poss\u00e8de une ressource sp\u00e9cifique appel\u00e9e Service . Service Le Service agit comme un Load Balancer qui va transmettre les requ\u00eates qu\u2019il re\u00e7oit vers les Pods auxquels il est rattach\u00e9. Ce rattachement se fait gr\u00e2ce aux Selector. Pour publier le Deployment unicorn-front-deployment de l'\u00e9tape 1, Editez le fichier unicorn-front-service.yaml et d\u00e9ployez le avec kubectl apply : apiVersion : v1 kind : Service metadata : name : unicorn-front-service spec : selector : app : unicorn-front ports : - protocol : TCP port : 80 targetPort : 80 Note spec.ports.port est le port sur lequel le service va \u00e9couter. spec.ports.targetPort est le port expos\u00e9 par le container sur le pod. Important : V\u00e9rifiez que le targetPort du service correspond bien au containerPort du deployment (ou du pod). Pour \u00e9viter de s'emm\u00ealer les pinceaux, vous pouvez aussi nommer le port dans votre deployment et l'appeler dans le service ( spec.template.spec.containers[].ports[].name ). Par d\u00e9faut, les services sont de type ClusterIp , c'est-\u00e0-dire qu'ils sont accessibles depuis l'int\u00e9rieur du cluster Kubernetes (et donc pas publiquement sur internet) mais seulement pour les autres applications qui tournent dans le m\u00eame cluster Kubernetes. Un comportement id\u00e9al pour un Backend ou une base de donn\u00e9es mais non adapt\u00e9 pour le Front d\u2019un portail Web qui doit \u00eatre accessible \u00e0 des utilisateurs. Pour exposer ces services, il existe plusieurs solutions. Voici les deux les plus courantes : * Soit faire un service de type NodePort (pour exposer le service sur l\u2019ensemble des serveurs Worker du cluster Kubernetes) ou LoadBalancer (pour provisionner un load balancer sur le cloud provider sur lequel il tourne (AWS, GCP, AZURE pour citer les principaux). * Soit faire un Ingress . Pour les Service Http / Https, c\u2019est la solution pr\u00e9conis\u00e9e car elle permet d'acc\u00e9der aux applications en utilisant une m\u00eame IP publique. L\u2019ingress est en fait un Reverse Proxy (voir Api Gateway) et constitue le point d\u2019entr\u00e9e de tous les applicatifs \u00e0 publier sur le Web. L\u2019ingress transmet ensuite les requ\u00eates au service de type ClusterIP. Pour r\u00e9sumer, voil\u00e0 l\u2019id\u00e9e : UTILISATEUR \u2192 INGRESS \u2192 SERVICE \u2192 PODS \u00c0 noter que l\u2019Ingress Controller utilise lui-m\u00eame un service de type LoadBalancer ou NodePort partag\u00e9 par tous les ingress qu\u2019il va contr\u00f4ler. Ingress Nous allons maintenant mettre en place un Ingress Si vous \u00eates curieux, vous aurez remarqu\u00e9 qu\u2019il y a d\u00e9j\u00e0 un IngressClass dans le Cluster qui a \u00e9t\u00e9 provisionn\u00e9. Il s\u2019agit d\u2019un Ingress Controller Nginx. Qui dit Reverse Proxy, dit bien s\u00fbr URL. En effet, le Reverse Proxy utilise un ensemble URL+Path pour savoir vers quel service il doit transmettre les requ\u00eates. Un DNS a d\u00e9j\u00e0 \u00e9t\u00e9 cr\u00e9\u00e9 et vous a \u00e9t\u00e9 transmis par mail. Note Le DNS devrait \u00eatre de la forme cequevousvoulez.${namespace}.takima.cloud Exemple pour le namespace johndoe : unicorn.johndoe.takima.cloud Editer un fichier unicorn-front-ingress.yaml et d\u00e9ployez-le sur kubernetes : apiVersion : networking.k8s.io/v1 kind : Ingress metadata : annotations : kubernetes.io/ingress.class : nginx name : unicorn-front-ingress spec : rules : - host : replace-with-your-url http : paths : - backend : service : name : unicorn-front-service port : number : 80 path : / pathType : Prefix Essayez d'acc\u00e9der au service. Check Vous devez avoir les fichiers suivants dans le dossier step-2 : . \u251c\u2500\u2500 unicorn-front-service.yaml \u2514\u2500\u2500 unicorn-front-ingress.yaml Bonus Comme la s\u00e9curit\u00e9 c\u2019est important, il est possible d'utiliser HTTPS/TLS, mais pas d'inqui\u00e9tude, les \u00e9ventuels probl\u00e8mes de certificats ont \u00e9t\u00e9 r\u00e9gl\u00e9s en amont. Info Nous avons mis deux cluster-issuers dans le Cluster : * Un appel\u00e9 letsencrypt-staging qui vous permet de tout tester avec des certificats auto-sign\u00e9s. * L'autre appel\u00e9 letsencrypt-prod qui vous permet de vraiment g\u00e9n\u00e9rer un certificat Let's Encrypt valid. Nous vous recommandons de d'abord faire vos tests avec letsencrypt-staging , puis de switcher sur letsencrypt-prod une fois que vous pouvez bien acc\u00e9der \u00e0 votre ressource depuis un navigateur. annotations : cert-manager.io/cluster-issuer : letsencrypt-staging kubernetes.io/tls-acme : 'true' kubernetes.io/ingress.class : nginx name : unicorn-front-ingress spec : rules : - host : replace-with-your-url http : paths : - backend : service : name : unicorn-front-service port : number : 80 path : / pathType : Prefix tls : - hosts : - replace-with-your-url secretName : unicorn-front-tls V\u00e9rifiez l\u2019acc\u00e8s en SSL Mise en situation Pour bien se rendre compte du fonctionnement du Load Balancing, un Webservice simple a \u00e9t\u00e9 pr\u00e9par\u00e9. Tout d'abord, commencez par nettoyer votre Namespace : kubectl delete --all deployments kubectl delete --all services kubectl delete --all ingress Editez un nouveau Deployment avec l\u2019image suivante pour avoir 3 Pods master3.takima.io:4567/master3/kubernetes-resources/hello_world:latest . Vous pouvez nommer le fichier hello-deployment.yaml Info Cette image contient un node qui expose ses services sur le port 3000 . Question Que se passe-t'il ? Pourquoi ? Vous utiliserez la plupart du temps des containers provenant de Container Registry priv\u00e9s, peu d\u2019entreprises exposant publiquement leurs containers. Il faut alors g\u00e9rer les acc\u00e8s au Registry priv\u00e9. Pour cela, il convient de cr\u00e9er une nouvelle ressource de type Secret dans Kubernetes. Cette ressource sera d\u00e9crite plus en d\u00e9tail ult\u00e9rieurement. Pour le moment, cr\u00e9ez la ressource avec les informations que vous avez re\u00e7ues par mail : kubectl create secret docker-registry auth-master3-registry --docker-server = master3.takima.io:4567 --docker-username = readregcred --docker-password = <PASSWORD> Puis modifiez votre Deployment pour indiquer que vous souhaitez utiliser ce Secret pour pull l\u2019image. Pour cela, \u00e9ditez votre yaml ajoutez y ce block au m\u00eame niveau que spec.template.spec : imagePullSecrets : - name : auth-master3-registry Une fois le Deployment r\u00e9alis\u00e9 (contr\u00f4lez que les Pods ont bien le status running), cr\u00e9ez un Service ainsi qu\u2019un Ingress pour acc\u00e9der \u00e0 la Web App (nouveau fichier hello-service.yaml et hello-ingress.yaml ). Si vous ne l'avez pas fait dans votre Deployment , scalez votre Deployment \u00e0 3. Question D\u00e9crivez ce que r\u00e9pond la Web App ? Actualisez votre page avec CTRL + F5. Que se passe-t-il ? Maintenant, nous allons ajouter des configs de variables d'environement, celles-ci pourront \u00eatre utilis\u00e9es dans les container (et donc dans les applicatifs qui tournent dessus). Ajoutez les variables suivantes dans le deployment (au niveau spec.template.spec.container.env ): - name : K8S_NODE_NAME valueFrom : fieldRef : fieldPath : spec.nodeName - name : K8S_POD_NAME valueFrom : fieldRef : fieldPath : metadata.name - name : K8S_POD_IP valueFrom : fieldRef : fieldPath : status.podIP Question Que constatez-vous sur le navigateur ? Check Vous devez avoir les nouveaux fichiers suivants dans le dossier step-2 : . \u251c\u2500\u2500 hello-deployement.yaml \u251c\u2500\u2500 hello-service.yaml \u2514\u2500\u2500 hello-ingress.yaml Etape 3 : ConfigMap/Secret La plupart du temps lorsque qu'une application est d\u00e9ploy\u00e9e, vous aurez besoin de lui fournir une configuration. Par exemple, vous lui indiquerez des valeurs pour les variables d\u2019environnement (de la m\u00eame mani\u00e8re que le .env du Docker Compose). Bien qu\u2019il soit possible de configurer cela \u201cen dur\u201d dans le Pod ou le Deployment, il est d'adage que le concept de hardcoder des configs envoie droit \u00e0 la catastrophe. Dans Kubernetes, une ressource est d\u00e9di\u00e9e pour \u00e9viter cette destin\u00e9e : il s\u2019agit du ConfigMap . La plupart du temps un ConfigMap d\u00e9finit un couple Key/Value qui sera ensuite utilis\u00e9 dans le Pod pour configurer des variables d'environnement. Mais vous pourrez aussi d\u00e9finir un fichier de configuration entier et le monter comme un volume dans votre Pod . Exemple: Quelque chose est volontairement cach\u00e9 dans la Web App de d\u00e9mo. En effet, il est possible de lui d\u00e9finir une variable d\u2019environnement nomm\u00e9e CUSTOM_COLOR pour forcer la couleur du background. Pour cela il faut: * Cr\u00e9er un ConfigMap en configurant cette variable, nouveau fichier hello-config.yaml . * Consommer la variable dans le Deployment . apiVersion : v1 kind : ConfigMap metadata : name : web-app data : # property-like keys; each key maps to a simple value color : \"#200\" Le bloc suivant est \u00e0 placer dans le Deployment au niveau du template container (au m\u00eame niveau que le name ou l\u2019image du container utilis\u00e9 dans le deployment spec.template.spec.container ). Attention \u00e0 l'indentation du yaml ! env : - name : CUSTOM_COLOR # Vrai key de la variable d'env. Peut \u00eatre diff\u00e9rent de la valeur dans le config map valueFrom : configMapKeyRef : name : web-app # Nom du configmap key : color # nom de la clef dans le config map Secret Un Secret s\u2019utilise comme un ConfigMap mais sera masqu\u00e9 dans le cluster Kubernetes et vous pourrez appliquer des droits diff\u00e9rents sur ces ressources pour avoir des restrictions d\u2019acc\u00e8s. D\u2019ailleurs, pour le cr\u00e9er, vous devez encoder les donn\u00e9es en Base64. Deux solutions : * Soit directement en Command line avec kubectl create kubectl create secret generic my-secret --from-literal = username = user --from-literal = password = 'test123*' Remarque : dans ce cas on ne passe pas les data en base 64, la commande s'occupe de cela. * Soit via un yaml, mais attention, dans ce cas les valeurs des data doivent \u00eatre en Base64. Il faut donc convertir les valeurs au pr\u00e9alable : echo -n 'user' | base64 dXNlcg == echo -n 'test123*' | base64 dGVzdDEyMyo = Puis, le yaml peut \u00eatre \u00e9dit\u00e9, nouveau fichier hello-secret.yaml : apiVersion : v1 kind : Secret metadata : name : hello-secret type : Opaque data : username : YWRtaW4= password : MWYyZDFlMmU2N2Rm Il ne reste plus qu'\u00e0 kubectl apply le fichier. V\u00e9rifier dans Lens par exemple que le secret est cr\u00e9\u00e9. Dans Lens vous pouvez visualiser le contenu du secret en cliquant sur l'oeil Check Vous devriez avoir les fichiers suivants dans le dossier step-3 : . \u251c\u2500\u2500 hello-config.yaml \u2514\u2500\u2500 hello-secret.yaml Bonus 1: Rendre sa couleur secr\u00e8te Afin d'\u00e9viter que votre couleur pr\u00e9f\u00e9r\u00e9e soit connue de tous, passez la variable du code couleur en mode Secret . Bonne chance ! Bonus 2: Mon pod est-il en vie ? Il peut arriver parfois que notre application d\u00e9marre et soit capable de fonctionner de mani\u00e8re norminale pendant un temps avant de voir son service se d\u00e9grader (apparition d'un deadlock qui emp\u00eache l'application de fonctionner normalement). Ce genre d'erreur peut survenir sans pour autant que votre application s'arr\u00eate d'elle m\u00eame. Dans ce cas l\u00e0, il convient de fournir \u00e0 kubernetes un moyen de savoir si l'application devrait \u00eatre red\u00e9marr\u00e9e ou non. Cela tombe bien puisque notre application hello-world propose un endpoint sur /health donnant l'information sur son \u00e9tat courant. Param\u00e9trez votre Deployment pour qu'il fasse un appel r\u00e9gulier sur ce fameux endpoint /health . Pour v\u00e9rifier que tout marche bien, vous pouvez changer l'\u00e9tat du healtcheck en faisant une requ\u00eate GET sur le endpoint /kill . Une fois fait, l'un de vos pods devraient red\u00e9marrer. Bonus 3: Let my kubernetes Scale !!! Comme promis, voici une mise en pratique du HPA , la mise \u00e0 l'\u00e9chelle automatique d'un d\u00e9ploiement. Pour cela, commencez par supprimmer vos ressources d\u00e9ployment existantes (pour avoir de la visibilit\u00e9). Note Le HPA se base sur les ressources request d\u00e9finies dans votre d\u00e9ploiement, en effet on va d\u00e9finir un pourcentage bas\u00e9 sur ces ressources. Nous vous conseillons donc de r\u00e9aliser le bonus de l'\u00e9tape 1, qui traite de cela avant de continuer. Nous avons pr\u00e9par\u00e9 pour vous un petit applicatif qui simule une charge \u00e0 chaque fois qu'il recoit une requ\u00eate http. Commencez par \u00e9diter et deployer le fichier hpa-deployment.yaml avec : image : master3.takima.io:4567/master3/kubernetes-resources/hpa:latest replicas \u00e0 1 nom hpa et label app: hpa les ressources suivantes : resources : limits : cpu : 500m requests : cpu : 200m la mise en place du imagePullSecrets car on pull un registry priv\u00e9. Editez et d\u00e9ployez en suite le service correspondant : - targetPort: 80 - Port: 80 - nom: hpa Voil\u00e0 le socle est pr\u00eat pour mettre \u00e0 l'\u00e9chelle notre application: Mettez en place le hpa sur le d\u00e9ploiement : kubectl autoscale deployment hpa --cpu-percent=50 --min=1 --max=10 Ici on demande de scaler l'application d\u00e8s que la ressource cpu d\u00e9passe 50% du request (ici donc 100m cpu). V\u00e9rifiez l'\u00e9tat du HPA (on peut aussi le voir dans LENS) laptop-3007:~$ kubectl get hpa NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE hpa Deployment/hpa 0%/50% 1 10 1 105s On voit qu'il n'est pas tr\u00e8s charg\u00e9. Mais cela ne va pas durer ! Pour g\u00e9n\u00e9rer du traffic rien de tel qu'un pod qui attaque le service, sur un autre terminal, lancez cette commande et laissez la tourner en fond: kubectl run -i --tty load-generator --rm --image=public.ecr.aws/hudsonbay/busybox:latest --restart=Never -- /bin/sh -c \"while sleep 0.01; do wget -q -O- http://hpa; done\" Puis, commencez \u00e0 observer la charge monter sur votre d\u00e9ploiement en lancant plusieurs kubectl get hpa ou avec un watch (ici a 160%) laptop-3007:~$ kubectl get hpa NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE hpa Deployment/hpa 160%/50% 1 10 1 108s Voici ce qui est attendu : On voit bien notre applicatif scaler \u00e0 6 pods. On le constate aussi si l'on fait un kubectl get pods d'ailleurs. De plus nous pouvons lancer un kubectl describe hpa hpa pour regarder les logs du hpa et ces \u00e9venements : Les logs parlent d'eux m\u00eames ! Maintenant, coupez le process qui tourne en fond sur votre deuxi\u00e8me terminal. On remarque que la charge a diminu\u00e9 avec un kubectl get hpa Et au bout de quelques minutes le hpa devrait downscale l'applicatif. Par d\u00e9fault, pour des raisons de stabilisation, le dowscaling ne se fait qu'apr\u00e8s 300 secondes sous le d\u00e9clencheur (dans notre cas 50% de cpu request). Attendez donc un peu et v\u00e9rifiez que le d\u00e9ploiement repasse \u00e0 1 replicas. Check Vous devriez avoir 2 nouveaux fichiers dans le dossier hpa : . \u251c\u2500\u2500 hpa-deployment.yaml \u2514\u2500\u2500 hpa-service.yaml \u00a9 Takima 2022","title":"Day 1 - Discovery"},{"location":"ch1-discover-kubernetes/#day-1-discover-kubernetes","text":"","title":"Day 1: Discover Kubernetes"},{"location":"ch1-discover-kubernetes/#initialisation-des-outils","text":"Vous avez d\u00fb recevoir un petit cadeau dans votre bo\u00eete mail nomm\u00e9 kubeconfig. Ce fichier est la clef d\u2019acc\u00e8s au cluster Kubernetes qui a \u00e9t\u00e9 sp\u00e9cialement provisionn\u00e9 pour que vous puissiez vous amuser \u00e0 d\u00e9ployer des ressources, publier des services et les scaler. Pour pouvoir l\u2019utiliser, vous devrez d\u2019abord installer kubectl. Il s\u2019agit de l'outil d\u2019administration en ligne de commande (CLI) pour manager Kubernetes.","title":"Initialisation des outils"},{"location":"ch1-discover-kubernetes/#installation-dun-editeur-de-code","text":"Note Il est conseill\u00e9 d'installer un \u00e9diteur de code complet pour l'\u00e9dition des fichiers tout au long de cette formation. Si vous n'en avez pas, vous pouvez utiliser VsCode .","title":"Installation d'un \u00e9diteur de code"},{"location":"ch1-discover-kubernetes/#installation-de-kubectl","text":"","title":"Installation de kubectl"},{"location":"ch1-discover-kubernetes/#linux-macos","text":"T\u00e9l\u00e9chargez le client : curl -LO https://storage.googleapis.com/kubernetes-release/release/ $( curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt ) /bin/linux/amd64/kubectl Le rendre ex\u00e9cutable : chmod +x ./kubectl D\u00e9placez le binaire dans votre PATH : sudo mv ./kubectl /usr/local/bin/kubectl Testez pour vous assurer que la version que vous avez install\u00e9e est \u00e0 jour : kubectl version --client","title":"Linux / MacOS"},{"location":"ch1-discover-kubernetes/#windows","text":"T\u00e9l\u00e9chargez le client : curl -LO https://storage.googleapis.com/kubernetes-release/release/v1.23.0/bin/windows/amd64/kubectl.exe Ajoutez l'ex\u00e9cutable dans le PATH. Tutoriel sous Windows 10 Testez pour vous assurer que la version que vous avez install\u00e9e est \u00e0 jour : kubectl version --client","title":"Windows"},{"location":"ch1-discover-kubernetes/#kubeconfig","text":"Votre client Kubectl est maintenant op\u00e9rationnel. Pour administrer un cluster Kubernetes, vous allez devoir utiliser les informations de connexion pr\u00e9sentes dans le fichier kubeconfig. Pour cela, vous devez simplement placer ce fichier dans $HOME/.kube/config . Question Quelles sont les informations que l'on retrouve dans ce fichier ?","title":"Kubeconfig"},{"location":"ch1-discover-kubernetes/#lens","text":"Lens est un IDE (Integrated Development Environment) qui permet d'administrer localement plusieurs clusters kubernetes : Installez l\u2019IDE : https://k8slens.dev/ (installez la version correspondante \u00e0 votre OS) Ouvrez l\u2019IDE : le login est inutile et vous pouvez directement cliquer sur \"Browse catalog\". Vous devriez y trouver votre .kube/config.","title":"Lens"},{"location":"ch1-discover-kubernetes/#premieres-commandes","text":"Super ! Vous \u00eates maintenant pr\u00eat \u00e0 utiliser Kubernetes. Comme tout bon outil d\u2019administration, kubectl propose un helper assez fourni. Vous pouvez y jeter un coup d'\u0153il : kubectl -h Pour commencer, il faut savoir que Kubernetes fonctionne avec des Namespaces pour s\u00e9parer de mani\u00e8re logique les ressources. Cela permet de s\u00e9parer les projets ou les environnements par exemple et de d\u00e9finir des droits utilisateur sur chacun d\u2019entre eux. Dans le cadre de cet atelier, vous aurez acc\u00e8s \u00e0 un Namespace personnel qui a pour libell\u00e9 votre username : si votre mail est toto@exemple.com, votre namespace se nommera toto. Maintenant, essayez de conna\u00eetre les pods qui tournent dans votre namespace : kubectl get pods -n votre_namespace Maintenant, essayez la m\u00eame commande sur un autre namespace : kubectl get pods -n default Question Quelle est la diff\u00e9rence ?","title":"Premi\u00e8res commandes"},{"location":"ch1-discover-kubernetes/#etape-1-premieres-ressources-podsreplicasetdeployment","text":"","title":"Etape 1 : Premi\u00e8res ressources : Pods/Replicaset/Deployment"},{"location":"ch1-discover-kubernetes/#pods","text":"Dans le monde Kubernetes, TOUT est ressource : un d\u00e9ploiement est une ressource, un pod est une ressource, une configuration est une ressource, un disque est une ressource, etc\u2026 Tous ces objets Kubernetes peuvent \u00eatre d\u00e9crits dans un fichier (souvent un fichier yaml). Le Pod est la ressource minimale et triviale qu\u2019il est possible de d\u00e9ployer dans Kubernetes en terme de ressources applicatives. Pour faire simple, un pod est une ressource qui h\u00e9berge un ou plusieurs containers. Comme pour un container d\u00e9marr\u00e9 avec Docker, vous devez d\u00e9finir une image Docker \u00e0 consommer, des variables, des volumes. Par exemple : kubectl run mynginx --image nginx Pour chaque ressource, nous proposerons un lien qui renvoit vers la documentation officielle de Kubernetes, qui est une source fiable et souvent utile pour se rensigner sur les diff\u00e9rentes caract\u00e9ristiques des ressources. Pour le Pod, c'est par ici : https://kubernetes.io/docs/concepts/workloads/pods/ Info Derri\u00e8re cette commande, un fichier yaml de type POD est g\u00e9n\u00e9r\u00e9 et est envoy\u00e9 au KubeApi pour la cr\u00e9ation. Observez la cr\u00e9ation du pod : kubectl get pods mynginx Vous pouvez aussi d\u00e9crire le fichier yaml qui d\u00e9fini le pod : kubectl get pods mynginx -o yaml Question Quelles sont les propri\u00e9t\u00e9s principales que l'on retrouve ? Vous retrouvez dans les root properties : apiVersion kind metadata spec Maintenant, d\u00e9truisez le pod : kubectl delete pods nom_du_pod Voila vous avez cr\u00e9\u00e9 et supprim\u00e9 votre premi\u00e8re ressource dans kubernetes. Ce n'est pas si compliqu\u00e9 n'est ce pas ?","title":"Pods"},{"location":"ch1-discover-kubernetes/#pod-yaml-definition","text":"Nous allons imaginer une application vide, qui s'appelle Unicorn. Pour l'instant, Unicorn n'est qu'un container Nginx tout vide, c'est un front . Par soucis de convention, nous vous proposons de cr\u00e9er un fichier unicorn-front-pod.yaml Note Comme dans kubernetes TOUT est ressource yaml nous te conseillons de cr\u00e9er un dossier et une arborescence pour pouvoir les \u00e9diter et les modifier. par exemple k8s-TP1/step-1 dans lequel vous placerez vos ressources yaml manipul\u00e9es. Pour appliquer une ressource kubernetes d\u00e9crite en tant que yaml la commande est toujours la m\u00eame : kubectl apply -f monfichier.yaml A vous de jouer. Essayez de cr\u00e9er la m\u00eame ressource avec un fichier yaml : Tip Pour avoir le yaml correspondant \u00e0 une ressource \u00e0 cr\u00e9er, vous pouvez utiliser la fonction dry-run, exemple : kubectl run mynginx2 --image nginx --dry-run = client -o yaml > unicorn-front-pod.yml #cette commande ne cr\u00e9era pas le pod dans kubernetes mais dumpera l'\u00e9quivalant du fichier yaml qui aurait \u00e9t\u00e9 envoy\u00e9 au KubeApi pour la cr\u00e9ation. c'est tr\u00e8s utiles pour avoir un template de ressource voulu et ne pas d\u00e9marrer avec un yaml vierge # Pour appliquer une ressource d\u00e9crite dans un yaml : kubectl apply -f unicorn-front-pod.yml Comme avec docker, dans kubernetes, vous pouvez acc\u00e9der \u00e0 votre container en mode ex\u00e9cution (remarquez que l\u2019on retrouve des commandes Docker) : kubectl exec nom_du_pod -it -- bash # -it pour interactive : permet d'avoir la main dans l'invite de commande Ou voir les logs : kubectl logs nom_du_pod D\u00e9truisez de nouveau le pod : kubectl delete pods nom_du_pod Info Attention, le pod ne s'appelle plus \u201cmynginx\u201d, v\u00e9rifiez le fichier .yaml g\u00e9n\u00e9r\u00e9 pr\u00e9c\u00e9demment.","title":"Pod Yaml Definition"},{"location":"ch1-discover-kubernetes/#replicaset","text":"Ok, c\u2019est bien : vous avez r\u00e9ussi \u00e0 d\u00e9ployer un container dans un Pod ! Mais qu'en est-il de la haute disponibilit\u00e9 ou de la scalabilit\u00e9 promise ? Pour scaler un applicatif, il faut tout simplement rajouter des Pods du m\u00eame applicatif. Vous pourriez cr\u00e9er \u00e0 la main plusieurs Pods avec la m\u00eame image mais cela ne serait pas tr\u00e8s pratique. Il est beaucoup plus efficace de les regrouper au sein d\u2019un groupe de ressources. C\u2019est dans cet objectif que Kubernetes d\u00e9finit les ReplicaSet permettant de contr\u00f4ler plusieurs pods d\u2019un m\u00eame applicatif (m\u00eame image Docker) partageant les m\u00eames configs, les m\u00eames propri\u00e9t\u00e9s. Voici un exemple de ressource ReplicaSet : apiVersion : apps/v1 kind : ReplicaSet metadata : name : unicorn-front-replicaset labels : app : unicorn-front spec : template : metadata : name : unicorn-front-pod labels : app : unicorn-front spec : containers : - name : unicorn-front image : nginx replicas : 3 selector : matchLabels : app : unicorn-front Question Que remarquez-vous dans la description des properties spec: template ? \u00c0 quoi sert le selector: matchLabels ? Editez le yaml unicorn-front-replicaset.yaml et d\u00e9ployez ce ReplicaSet sur votre cluster avec kubectl apply . Question Combien y a-t'il de pods d\u00e9ploy\u00e9s dans votre namespace ? Maintenant, supprimez un Pod . Question Que se passe-t'il ? Supprimez le ReplicaSet . Question Que se passe-t'il ?","title":"Replicaset"},{"location":"ch1-discover-kubernetes/#deployment","text":"Une ressource Kubernetes de type Deployment permet de facilement piloter des ReplicaSet`. En pratique, c'est typiquement avec cette ressource que sont d\u00e9ploy\u00e9es les applications. En effet, le Deployment g\u00e8re la notion de cycle de vie des Pods (via les version de ReplicaSet pilot\u00e9s), notamment avec la d\u00e9finition de RollingUpgrade lors d\u2019un d\u00e9ploiement de nouvelles versions. Cependant, il faut que la strat\u00e9gie par d\u00e9faut de RollingUpdate soit conserv\u00e9e ( .spec.strategy.type ) : cela facilite les rollbacks vers une version ant\u00e9rieure. Lors de la modification de la version d\u2019une image Docker consomm\u00e9e dans un d\u00e9ploiement, un nouveau ReplicaSet sera cr\u00e9\u00e9. L\u2019ancien ReplicaSet correspondant \u00e0 l'ancienne version de l\u2019image reste et les Pods sont vid\u00e9s de l\u2019ancien ReplicaSet vers le nouveau ( RollingUpgrade ). Ce transfert se fait au \u201cfil de l'eau\u201d, de mani\u00e8re \u00e0 ce qu\u2019il n\u2019y ait pas d\u2019interruption de service (par d\u00e9faut Kubernetes tente de garder 75% des pods actifs). Dans le cas du TP, il ne d\u00e9truit les Pods obsol\u00e8tes qu\u2019apr\u00e8s que le nouveau Pod obtienne le statut Running. De cette mani\u00e8re, il est possible de facilement Rollback (processus inverse, les Pods allant du nouveau ReplicaSet vers l\u2019ancien). La cr\u00e9ation d\u2019un Deployment entra\u00eene la cr\u00e9ation d\u2019un ReplicaSet et donc la cr\u00e9ation d\u2019un ou plusieurs Pods. La ressource Deployment est tr\u00e8s proche de celle d\u2019un ReplicaSet : apiVersion : apps/v1 kind : Deployment metadata : name : unicorn-front-deployment labels : app : unicorn-front spec : replicas : 3 selector : matchLabels : app : unicorn-front template : metadata : labels : app : unicorn-front spec : containers : - name : unicorn-front image : nginx:1.7.9 ports : - containerPort : 80 Question Quels sont les changements par rapport au ReplicaSet ? Editer un yaml unicorn-front-deployment.yaml et d\u00e9ployez ce Deployment . Question Combien y a-t'il de ReplicaSet ? De Pods ? Commandes utiles : kubectl get all kubectl rollout status deployment.v1.apps/unicorn-front-deployment Maintenant, la notion d\u2019upgrade : c\u2019est en pratique ce qu\u2019il se passe lorsque l\u2019on r\u00e9alise un d\u00e9ploiement applicatif d\u2019une nouvelle release (via un Continuous Deployment par exemple). Pour cela, changez la version de l\u2019image nginx : Modifier une image docker dans un deployment revient a modifier cette ressource deployment. Pour modifier une ressource, vous avez 3 solutions : kubectl set ou kubectl patch kubectl set image deployment/unicorn-front-deployment nginx = nginx:1.9.1 --record Editer le fichier yaml en local et faire un kubectl apply -f monfichier.yaml Il est possible \u00e9galement d'\u00e9diter le yaml d\u00e9finition de d\u00e9ploiement en changeant l\u2019image et d'utiliser cette commande : kubectl apply -f ... Editer directement la ressouce avec kubectl edit kubectl edit deployment.v1.apps/unicorn-front-deployment Info On preferera r\u00e9aliser des updates de ressources en \u00e9ditant son fichier yaml (ici unicorn-front-deployment.yaml) en utilisant donc la 2\u00e8me solution. Cela permet d'avoir la bonne version dans son dossier (fichier synchronis\u00e9 avec ce qui est sur le cluster). Regardez le statut du RollingUpgrade : kubectl rollout status deployment.v1.apps/unicorn-front-deployment Si vous utilisez la commande rapidement, vous verrez le RollingUpgrade en cours. Question Une fois termin\u00e9, combien y a-t-il de replicaset ? Combien y a-t-il de Pods ? Allez voir les logs des \u00e9v\u00e9nements du d\u00e9ploiement avec kubectl describe deployments . Qu\u2019observez vous ? Commande utile : kubectl describe deployments","title":"Deployment"},{"location":"ch1-discover-kubernetes/#faire-un-rollback","text":"Mettez \u00e0 jour votre d\u00e9ploiement avec une nouvelle version de l\u2019image nginx. kubectl set image deployment.v1.apps/unicorn-front-deployment nginx = nginx:1.91-falseimage --record = true Observez votre Deployment , vos Pods et vos ReplicaSets . Question Que se passe-t'il ? Pourquoi ? Dans ce cas de figure, l'objectif est de revenir \u00e0 une version stable de Deployment (une version fonctionnelle avec une bonne image nginx). Pour commencer, v\u00e9rifiez les r\u00e9visions de ce d\u00e9ploiement : kubectl rollout history deployment.v1.apps/unicorn-front-deployment Question Combien y a-t-il de r\u00e9visions ? \u00c0 quoi correspond le champ CHANGE-CAUSE ? Vous pouvez afficher les d\u00e9tails de chaque r\u00e9vision (pour la r\u00e9vision 2 par exemple) avec cette commande : kubectl rollout history deployment.v1.apps/unicorn-front-deployment --revision = 2 Maintenant que la version stable est identifi\u00e9e, voici la commande pour retourner \u00e0 cette version : kubectl rollout undo deployment.v1.apps/unicorn-front-deployment --to-revision = 2 Info Pour revenir \u00e0 la version pr\u00e9c\u00e9dente, il est aussi possible d'utiliser cette commade : kubectl rollout undo deployment.v1.apps/unicorn-front-deployment","title":"Faire un Rollback"},{"location":"ch1-discover-kubernetes/#mettre-a-lechelle","text":"Le d\u00e9ploiement de nouvelles versions (releases) est maintenant ma\u00eetris\u00e9 mais dans l'hypoth\u00e8se que le site internet est promu au journal t\u00e9l\u00e9vis\u00e9 et qu\u2019un afflux cons\u00e9quent de visiteurs se connectent dessus, que faire ? Il faut tout simplement d\u00e9ployer plus de Pods de l'applicatif. C\u2019est ce qui est appell\u00e9 scaler une application. Scalez le serveur nginx \u00e0 5 : kubectl scale deployment.v1.apps/unicorn-front-deployment --replicas = 5 Info Encore une fois cette commande est pratique pour le cadre du TP mais en pratique on pref\u00e9rera \u00e9diter son fichier yaml de d\u00e9ploiment et mettre replica: 5 puis faire un kubectl apply -f unicorn-front-deployment.yaml Question Combien y a-t'il de Pods ? Info Pour aller plus loin, Kubernetes impl\u00e9mente, si c\u2019est activ\u00e9, un syst\u00e8me d\u2019autoscaling appel\u00e9 HPA ( Horizontal Pods Autoscaling ). Kubernetes se chargera de scaler automatiquement l'application en fonction de trigger d\u00e9finis par exemple sur la consommation CPU ou la RAM. Si vous \u00eates rapide une impl\u00e9mentation du HPA est propos\u00e9 en bonus.","title":"Mettre \u00e0 l'\u00e9chelle"},{"location":"ch1-discover-kubernetes/#mettre-en-standby-un-deploiement","text":"Mettez en pause un Deployment : kubectl rollout pause deployment.v1.apps/unicorn-front-deployment kubectl set image deployment.v1.apps/unicorn-front-deployment nginx = nginx:1.20.2 Question Que se passe-t-il au niveau ReplicaSet ? kubectl rollout resume deployment.v1.apps/unicorn-front-deployment Question Que se passe-t-il au niveau ReplicaSet ? Info La pause permet de modifier le d\u00e9ploiement (encore une autre version, des properties du Deployment comme les limits/ressources) sans d\u00e9clencher de RollingUpgrade .","title":"Mettre en standby un deploiement"},{"location":"ch1-discover-kubernetes/#bonus","text":"Lorsque des applications sont d\u00e9ploy\u00e9es sous forme de containers (dans des Pods), il est indispensable de contr\u00f4ler les ressources qu\u2019elles consomment. Imaginez qu\u2019un Pod se mette \u00e0 consommer plusieurs dizaines de gigabytes de m\u00e9moire vive, quel serait l\u2019impact sur les autres Pods tournant sur le m\u00eame Worker Node (m\u00eame serveur) ? Pour emp\u00eacher cela, Kubernetes impl\u00e9mente la notion de Ressources Management. Example resources : requests : memory : \"64Mi\" cpu : \"250m\" limits : memory : \"128Mi\" cpu : \"500m\" Ce block se place au niveau spec.container * Le requests est la ressource monopolis\u00e9e au start du Container. * La limits est le maximum de m\u00e9moire et CPU consommable par le Pod. Impl\u00e9mentez une ressource management pour votre Deployment avec ces sp\u00e9cifications : * Requests : m\u00e9moire : 32 Mo / CPU : 100m * Limits : m\u00e9moire 256 Mo / CPU : 400m Check Vous devez avoir les fichiers suivants dans le dossier step-1 : . \u251c\u2500\u2500 unicorn-front-deployment.yaml \u251c\u2500\u2500 unicorn-front-replicaset.yaml \u2514\u2500\u2500 unicorn-front-pod.yaml","title":"Bonus"},{"location":"ch1-discover-kubernetes/#etape-2-publication-serviceingress","text":"Note Vous pouvez maintenant vous placer dans le dossier k8s-TP1/step-2 dans lequel vous placerez vos ressources yaml manipul\u00e9es pendant cette \u00e9tape. Gardez les ressources yaml du step-1 qui pourront vous servir de r\u00e9f\u00e9rence. La plupart du temps l'applicatif qui tourne dans un Pod Kubernetes doit d\u00e9livrer un service (que ce soit une Api, un Front ou une base de donn\u00e9es, par exemple) et doit donc \u00eatre requ\u00eat\u00e9. Kubernetes poss\u00e8de une ressource sp\u00e9cifique appel\u00e9e Service .","title":"Etape 2 : Publication Service/Ingress"},{"location":"ch1-discover-kubernetes/#service","text":"Le Service agit comme un Load Balancer qui va transmettre les requ\u00eates qu\u2019il re\u00e7oit vers les Pods auxquels il est rattach\u00e9. Ce rattachement se fait gr\u00e2ce aux Selector. Pour publier le Deployment unicorn-front-deployment de l'\u00e9tape 1, Editez le fichier unicorn-front-service.yaml et d\u00e9ployez le avec kubectl apply : apiVersion : v1 kind : Service metadata : name : unicorn-front-service spec : selector : app : unicorn-front ports : - protocol : TCP port : 80 targetPort : 80 Note spec.ports.port est le port sur lequel le service va \u00e9couter. spec.ports.targetPort est le port expos\u00e9 par le container sur le pod. Important : V\u00e9rifiez que le targetPort du service correspond bien au containerPort du deployment (ou du pod). Pour \u00e9viter de s'emm\u00ealer les pinceaux, vous pouvez aussi nommer le port dans votre deployment et l'appeler dans le service ( spec.template.spec.containers[].ports[].name ). Par d\u00e9faut, les services sont de type ClusterIp , c'est-\u00e0-dire qu'ils sont accessibles depuis l'int\u00e9rieur du cluster Kubernetes (et donc pas publiquement sur internet) mais seulement pour les autres applications qui tournent dans le m\u00eame cluster Kubernetes. Un comportement id\u00e9al pour un Backend ou une base de donn\u00e9es mais non adapt\u00e9 pour le Front d\u2019un portail Web qui doit \u00eatre accessible \u00e0 des utilisateurs. Pour exposer ces services, il existe plusieurs solutions. Voici les deux les plus courantes : * Soit faire un service de type NodePort (pour exposer le service sur l\u2019ensemble des serveurs Worker du cluster Kubernetes) ou LoadBalancer (pour provisionner un load balancer sur le cloud provider sur lequel il tourne (AWS, GCP, AZURE pour citer les principaux). * Soit faire un Ingress . Pour les Service Http / Https, c\u2019est la solution pr\u00e9conis\u00e9e car elle permet d'acc\u00e9der aux applications en utilisant une m\u00eame IP publique. L\u2019ingress est en fait un Reverse Proxy (voir Api Gateway) et constitue le point d\u2019entr\u00e9e de tous les applicatifs \u00e0 publier sur le Web. L\u2019ingress transmet ensuite les requ\u00eates au service de type ClusterIP. Pour r\u00e9sumer, voil\u00e0 l\u2019id\u00e9e : UTILISATEUR \u2192 INGRESS \u2192 SERVICE \u2192 PODS \u00c0 noter que l\u2019Ingress Controller utilise lui-m\u00eame un service de type LoadBalancer ou NodePort partag\u00e9 par tous les ingress qu\u2019il va contr\u00f4ler.","title":"Service"},{"location":"ch1-discover-kubernetes/#ingress","text":"Nous allons maintenant mettre en place un Ingress Si vous \u00eates curieux, vous aurez remarqu\u00e9 qu\u2019il y a d\u00e9j\u00e0 un IngressClass dans le Cluster qui a \u00e9t\u00e9 provisionn\u00e9. Il s\u2019agit d\u2019un Ingress Controller Nginx. Qui dit Reverse Proxy, dit bien s\u00fbr URL. En effet, le Reverse Proxy utilise un ensemble URL+Path pour savoir vers quel service il doit transmettre les requ\u00eates. Un DNS a d\u00e9j\u00e0 \u00e9t\u00e9 cr\u00e9\u00e9 et vous a \u00e9t\u00e9 transmis par mail. Note Le DNS devrait \u00eatre de la forme cequevousvoulez.${namespace}.takima.cloud Exemple pour le namespace johndoe : unicorn.johndoe.takima.cloud Editer un fichier unicorn-front-ingress.yaml et d\u00e9ployez-le sur kubernetes : apiVersion : networking.k8s.io/v1 kind : Ingress metadata : annotations : kubernetes.io/ingress.class : nginx name : unicorn-front-ingress spec : rules : - host : replace-with-your-url http : paths : - backend : service : name : unicorn-front-service port : number : 80 path : / pathType : Prefix Essayez d'acc\u00e9der au service. Check Vous devez avoir les fichiers suivants dans le dossier step-2 : . \u251c\u2500\u2500 unicorn-front-service.yaml \u2514\u2500\u2500 unicorn-front-ingress.yaml","title":"Ingress"},{"location":"ch1-discover-kubernetes/#bonus_1","text":"Comme la s\u00e9curit\u00e9 c\u2019est important, il est possible d'utiliser HTTPS/TLS, mais pas d'inqui\u00e9tude, les \u00e9ventuels probl\u00e8mes de certificats ont \u00e9t\u00e9 r\u00e9gl\u00e9s en amont. Info Nous avons mis deux cluster-issuers dans le Cluster : * Un appel\u00e9 letsencrypt-staging qui vous permet de tout tester avec des certificats auto-sign\u00e9s. * L'autre appel\u00e9 letsencrypt-prod qui vous permet de vraiment g\u00e9n\u00e9rer un certificat Let's Encrypt valid. Nous vous recommandons de d'abord faire vos tests avec letsencrypt-staging , puis de switcher sur letsencrypt-prod une fois que vous pouvez bien acc\u00e9der \u00e0 votre ressource depuis un navigateur. annotations : cert-manager.io/cluster-issuer : letsencrypt-staging kubernetes.io/tls-acme : 'true' kubernetes.io/ingress.class : nginx name : unicorn-front-ingress spec : rules : - host : replace-with-your-url http : paths : - backend : service : name : unicorn-front-service port : number : 80 path : / pathType : Prefix tls : - hosts : - replace-with-your-url secretName : unicorn-front-tls V\u00e9rifiez l\u2019acc\u00e8s en SSL","title":"Bonus"},{"location":"ch1-discover-kubernetes/#mise-en-situation","text":"Pour bien se rendre compte du fonctionnement du Load Balancing, un Webservice simple a \u00e9t\u00e9 pr\u00e9par\u00e9. Tout d'abord, commencez par nettoyer votre Namespace : kubectl delete --all deployments kubectl delete --all services kubectl delete --all ingress Editez un nouveau Deployment avec l\u2019image suivante pour avoir 3 Pods master3.takima.io:4567/master3/kubernetes-resources/hello_world:latest . Vous pouvez nommer le fichier hello-deployment.yaml Info Cette image contient un node qui expose ses services sur le port 3000 . Question Que se passe-t'il ? Pourquoi ? Vous utiliserez la plupart du temps des containers provenant de Container Registry priv\u00e9s, peu d\u2019entreprises exposant publiquement leurs containers. Il faut alors g\u00e9rer les acc\u00e8s au Registry priv\u00e9. Pour cela, il convient de cr\u00e9er une nouvelle ressource de type Secret dans Kubernetes. Cette ressource sera d\u00e9crite plus en d\u00e9tail ult\u00e9rieurement. Pour le moment, cr\u00e9ez la ressource avec les informations que vous avez re\u00e7ues par mail : kubectl create secret docker-registry auth-master3-registry --docker-server = master3.takima.io:4567 --docker-username = readregcred --docker-password = <PASSWORD> Puis modifiez votre Deployment pour indiquer que vous souhaitez utiliser ce Secret pour pull l\u2019image. Pour cela, \u00e9ditez votre yaml ajoutez y ce block au m\u00eame niveau que spec.template.spec : imagePullSecrets : - name : auth-master3-registry Une fois le Deployment r\u00e9alis\u00e9 (contr\u00f4lez que les Pods ont bien le status running), cr\u00e9ez un Service ainsi qu\u2019un Ingress pour acc\u00e9der \u00e0 la Web App (nouveau fichier hello-service.yaml et hello-ingress.yaml ). Si vous ne l'avez pas fait dans votre Deployment , scalez votre Deployment \u00e0 3. Question D\u00e9crivez ce que r\u00e9pond la Web App ? Actualisez votre page avec CTRL + F5. Que se passe-t-il ? Maintenant, nous allons ajouter des configs de variables d'environement, celles-ci pourront \u00eatre utilis\u00e9es dans les container (et donc dans les applicatifs qui tournent dessus). Ajoutez les variables suivantes dans le deployment (au niveau spec.template.spec.container.env ): - name : K8S_NODE_NAME valueFrom : fieldRef : fieldPath : spec.nodeName - name : K8S_POD_NAME valueFrom : fieldRef : fieldPath : metadata.name - name : K8S_POD_IP valueFrom : fieldRef : fieldPath : status.podIP Question Que constatez-vous sur le navigateur ? Check Vous devez avoir les nouveaux fichiers suivants dans le dossier step-2 : . \u251c\u2500\u2500 hello-deployement.yaml \u251c\u2500\u2500 hello-service.yaml \u2514\u2500\u2500 hello-ingress.yaml","title":"Mise en situation"},{"location":"ch1-discover-kubernetes/#etape-3-configmapsecret","text":"La plupart du temps lorsque qu'une application est d\u00e9ploy\u00e9e, vous aurez besoin de lui fournir une configuration. Par exemple, vous lui indiquerez des valeurs pour les variables d\u2019environnement (de la m\u00eame mani\u00e8re que le .env du Docker Compose). Bien qu\u2019il soit possible de configurer cela \u201cen dur\u201d dans le Pod ou le Deployment, il est d'adage que le concept de hardcoder des configs envoie droit \u00e0 la catastrophe. Dans Kubernetes, une ressource est d\u00e9di\u00e9e pour \u00e9viter cette destin\u00e9e : il s\u2019agit du ConfigMap . La plupart du temps un ConfigMap d\u00e9finit un couple Key/Value qui sera ensuite utilis\u00e9 dans le Pod pour configurer des variables d'environnement. Mais vous pourrez aussi d\u00e9finir un fichier de configuration entier et le monter comme un volume dans votre Pod .","title":"Etape 3 : ConfigMap/Secret"},{"location":"ch1-discover-kubernetes/#exemple","text":"Quelque chose est volontairement cach\u00e9 dans la Web App de d\u00e9mo. En effet, il est possible de lui d\u00e9finir une variable d\u2019environnement nomm\u00e9e CUSTOM_COLOR pour forcer la couleur du background. Pour cela il faut: * Cr\u00e9er un ConfigMap en configurant cette variable, nouveau fichier hello-config.yaml . * Consommer la variable dans le Deployment . apiVersion : v1 kind : ConfigMap metadata : name : web-app data : # property-like keys; each key maps to a simple value color : \"#200\" Le bloc suivant est \u00e0 placer dans le Deployment au niveau du template container (au m\u00eame niveau que le name ou l\u2019image du container utilis\u00e9 dans le deployment spec.template.spec.container ). Attention \u00e0 l'indentation du yaml ! env : - name : CUSTOM_COLOR # Vrai key de la variable d'env. Peut \u00eatre diff\u00e9rent de la valeur dans le config map valueFrom : configMapKeyRef : name : web-app # Nom du configmap key : color # nom de la clef dans le config map","title":"Exemple:"},{"location":"ch1-discover-kubernetes/#secret","text":"Un Secret s\u2019utilise comme un ConfigMap mais sera masqu\u00e9 dans le cluster Kubernetes et vous pourrez appliquer des droits diff\u00e9rents sur ces ressources pour avoir des restrictions d\u2019acc\u00e8s. D\u2019ailleurs, pour le cr\u00e9er, vous devez encoder les donn\u00e9es en Base64. Deux solutions : * Soit directement en Command line avec kubectl create kubectl create secret generic my-secret --from-literal = username = user --from-literal = password = 'test123*' Remarque : dans ce cas on ne passe pas les data en base 64, la commande s'occupe de cela. * Soit via un yaml, mais attention, dans ce cas les valeurs des data doivent \u00eatre en Base64. Il faut donc convertir les valeurs au pr\u00e9alable : echo -n 'user' | base64 dXNlcg == echo -n 'test123*' | base64 dGVzdDEyMyo = Puis, le yaml peut \u00eatre \u00e9dit\u00e9, nouveau fichier hello-secret.yaml : apiVersion : v1 kind : Secret metadata : name : hello-secret type : Opaque data : username : YWRtaW4= password : MWYyZDFlMmU2N2Rm Il ne reste plus qu'\u00e0 kubectl apply le fichier. V\u00e9rifier dans Lens par exemple que le secret est cr\u00e9\u00e9. Dans Lens vous pouvez visualiser le contenu du secret en cliquant sur l'oeil Check Vous devriez avoir les fichiers suivants dans le dossier step-3 : . \u251c\u2500\u2500 hello-config.yaml \u2514\u2500\u2500 hello-secret.yaml","title":"Secret"},{"location":"ch1-discover-kubernetes/#bonus-1-rendre-sa-couleur-secrete","text":"Afin d'\u00e9viter que votre couleur pr\u00e9f\u00e9r\u00e9e soit connue de tous, passez la variable du code couleur en mode Secret . Bonne chance !","title":"Bonus 1: Rendre sa couleur secr\u00e8te"},{"location":"ch1-discover-kubernetes/#bonus-2-mon-pod-est-il-en-vie","text":"Il peut arriver parfois que notre application d\u00e9marre et soit capable de fonctionner de mani\u00e8re norminale pendant un temps avant de voir son service se d\u00e9grader (apparition d'un deadlock qui emp\u00eache l'application de fonctionner normalement). Ce genre d'erreur peut survenir sans pour autant que votre application s'arr\u00eate d'elle m\u00eame. Dans ce cas l\u00e0, il convient de fournir \u00e0 kubernetes un moyen de savoir si l'application devrait \u00eatre red\u00e9marr\u00e9e ou non. Cela tombe bien puisque notre application hello-world propose un endpoint sur /health donnant l'information sur son \u00e9tat courant. Param\u00e9trez votre Deployment pour qu'il fasse un appel r\u00e9gulier sur ce fameux endpoint /health . Pour v\u00e9rifier que tout marche bien, vous pouvez changer l'\u00e9tat du healtcheck en faisant une requ\u00eate GET sur le endpoint /kill . Une fois fait, l'un de vos pods devraient red\u00e9marrer.","title":"Bonus 2: Mon pod est-il en vie ?"},{"location":"ch1-discover-kubernetes/#bonus-3-let-my-kubernetes-scale","text":"Comme promis, voici une mise en pratique du HPA , la mise \u00e0 l'\u00e9chelle automatique d'un d\u00e9ploiement. Pour cela, commencez par supprimmer vos ressources d\u00e9ployment existantes (pour avoir de la visibilit\u00e9). Note Le HPA se base sur les ressources request d\u00e9finies dans votre d\u00e9ploiement, en effet on va d\u00e9finir un pourcentage bas\u00e9 sur ces ressources. Nous vous conseillons donc de r\u00e9aliser le bonus de l'\u00e9tape 1, qui traite de cela avant de continuer. Nous avons pr\u00e9par\u00e9 pour vous un petit applicatif qui simule une charge \u00e0 chaque fois qu'il recoit une requ\u00eate http. Commencez par \u00e9diter et deployer le fichier hpa-deployment.yaml avec : image : master3.takima.io:4567/master3/kubernetes-resources/hpa:latest replicas \u00e0 1 nom hpa et label app: hpa les ressources suivantes : resources : limits : cpu : 500m requests : cpu : 200m la mise en place du imagePullSecrets car on pull un registry priv\u00e9. Editez et d\u00e9ployez en suite le service correspondant : - targetPort: 80 - Port: 80 - nom: hpa Voil\u00e0 le socle est pr\u00eat pour mettre \u00e0 l'\u00e9chelle notre application: Mettez en place le hpa sur le d\u00e9ploiement : kubectl autoscale deployment hpa --cpu-percent=50 --min=1 --max=10 Ici on demande de scaler l'application d\u00e8s que la ressource cpu d\u00e9passe 50% du request (ici donc 100m cpu). V\u00e9rifiez l'\u00e9tat du HPA (on peut aussi le voir dans LENS) laptop-3007:~$ kubectl get hpa NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE hpa Deployment/hpa 0%/50% 1 10 1 105s On voit qu'il n'est pas tr\u00e8s charg\u00e9. Mais cela ne va pas durer ! Pour g\u00e9n\u00e9rer du traffic rien de tel qu'un pod qui attaque le service, sur un autre terminal, lancez cette commande et laissez la tourner en fond: kubectl run -i --tty load-generator --rm --image=public.ecr.aws/hudsonbay/busybox:latest --restart=Never -- /bin/sh -c \"while sleep 0.01; do wget -q -O- http://hpa; done\" Puis, commencez \u00e0 observer la charge monter sur votre d\u00e9ploiement en lancant plusieurs kubectl get hpa ou avec un watch (ici a 160%) laptop-3007:~$ kubectl get hpa NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE hpa Deployment/hpa 160%/50% 1 10 1 108s Voici ce qui est attendu : On voit bien notre applicatif scaler \u00e0 6 pods. On le constate aussi si l'on fait un kubectl get pods d'ailleurs. De plus nous pouvons lancer un kubectl describe hpa hpa pour regarder les logs du hpa et ces \u00e9venements : Les logs parlent d'eux m\u00eames ! Maintenant, coupez le process qui tourne en fond sur votre deuxi\u00e8me terminal. On remarque que la charge a diminu\u00e9 avec un kubectl get hpa Et au bout de quelques minutes le hpa devrait downscale l'applicatif. Par d\u00e9fault, pour des raisons de stabilisation, le dowscaling ne se fait qu'apr\u00e8s 300 secondes sous le d\u00e9clencheur (dans notre cas 50% de cpu request). Attendez donc un peu et v\u00e9rifiez que le d\u00e9ploiement repasse \u00e0 1 replicas. Check Vous devriez avoir 2 nouveaux fichiers dans le dossier hpa : . \u251c\u2500\u2500 hpa-deployment.yaml \u2514\u2500\u2500 hpa-service.yaml \u00a9 Takima 2022","title":"Bonus 3: Let my kubernetes Scale !!!"},{"location":"ch2-deep-dive/","text":"TP initiation Kubernetes (J2) Objectifs Vous savez dor\u00e9navant d\u00e9ployer et administrer un applicatif simple sur Kubernetes. Mais les applicatifs ne se limitent souvent pas qu'\u00e0 un simple Front. Sur les projets, vous aurez par exemple besoin de cr\u00e9er des APIs, des bases de donn\u00e9es, que vos donn\u00e9es soient persistentes, de g\u00e9rer des brokers, etc. L'architecture la plus simple que l'on retrouve dans la majorit\u00e9 des projets est l'architecture 3 tiers. Elle contient les \u00e9l\u00e9ments suivants : - Un front - Une API - Une base de donn\u00e9es Aujourd'hui, l'objectif est de mettre en application ce qui a \u00e9t\u00e9 appris pour d\u00e9ployer un applicatif 3 tiers dans Kubernetes. Rien que \u00e7a ! Etant donn\u00e9 que chaque ressource Kubernetes correspond \u00e0 un fichier yaml, vous allez utiliser une arboresence de dossiers pr\u00e9cise et cr\u00e9er les dossiers suivants pour bien organiser vos fichiers : . \u2514\u2500\u2500 TP-kube-02 \u251c\u2500\u2500 api \u251c\u2500\u2500 database \u2514\u2500\u2500 front Pour la suite du TP, un minimum de ressources concernant chaque service sera fourni et \u00e7e sera \u00e0 vous de cr\u00e9er les ressources Kubernetes n\u00e9cessaires ! Amusez-vous bien ! 1\u00e8re \u00e9tape : d\u00e9ployer l\u2019API Ressources n\u00e9cessaires L\u2019image API \u00e0 utiliser est la suivante : master3.takima.io:4567/master3/kubernetes-resources/api:latest Info L'image contient une API Java Spring Boot et expose ses services sur le port 8080 . \u00c0 vous de jouer Cr\u00e9ez le fichier api-deployment.yaml associ\u00e9 \u00e0 cette image et d\u00e9ployez-le. Cr\u00e9ez le fichier api-service.yaml associ\u00e9 au Deployment et d\u00e9ployez-le. Cr\u00e9ez le fichier api-ingress.yaml pour acc\u00e9der \u00e0 ce service et d\u00e9ployez-le. Question Que se passe-t-il au niveau des Pods de l\u2019API ? Vous pouvez jeter un oeil aux logs. ( kubectl logs -f nomdupod ) L\u2019API a donc besoin d\u2019une base de donn\u00e9es pour fonctionner. Vous vous doutez maintenant \u00e0 quoi servent les variables d'environnements utilisables , nous les utiliserons plus tard. Check Vous devez avoir les fichiers suivants dans le dossier api : . \u251c\u2500\u2500 api-deployment.yaml \u251c\u2500\u2500 api-ingress.yaml \u2514\u2500\u2500 api-service.yaml 2\u1d49 \u00e9tape : cr\u00e9er la base de donn\u00e9es Comme l'API a besoin d\u2019une base de donn\u00e9es, il faut lui en fournir une. L\u2019API utilise un SGBDR (Syst\u00e8me de Gestion de Base de Donn\u00e9es Relationnelles) Postgres. Ressources n\u00e9cessaires Rappel des variables d'environnements disponibles sur API : DB_ENDPOINT POSTGRES_DB POSTGRES_USER POSTGRES_PASSWORD Image de la base de donn\u00e9es : master3.takima.io:4567/master3/kubernetes-resources/db:latest Info Postgres est accessible depuis le port 5432 . Variables d'environnement utilisables : POSTGRES_PASSWORD POSTGRES_USER POSTGRES_DB \u00c0 vous de jouer Cr\u00e9ez la ressource de type Secret : pg-credentials.yaml nomm\u00e9 \u201cpg-credentials\u201d contenant le username et le password (choisissez celui que vous souhaitez utiliser mais attention, n\u2019oubliez pas d'encoder les valeurs en base64, comme lors de la fin du tp 1). Cr\u00e9ez un Configmap : pg-config.yaml nomm\u00e9 \u201cpg-config\u201d contenant le nom de la base de donn\u00e9es, nommer la base de donn\u00e9es \u201ccdb-db\u201d. Cr\u00e9ez le Deployment : pg-deployment.yaml associ\u00e9 \u00e0 l'image de la base de donn\u00e9es. Tip Bien penser \u00e0 mapper les variables d'environnement dans le Deployment avec le ConfigMap et le Secret. Penser aux credentials pour pouvoir pull sur le registry priv\u00e9 master3.takima.io. Cr\u00e9ez le Service associ\u00e9 au Deployment : pg-service.yaml (le container \u00e9coute sur le port 5432). Pour v\u00e9rifier que la base de donn\u00e9es fonctionne, vous pouvez afficher les logs et constater qu\u2019elle est bien d\u00e9marr\u00e9e. Check Vous devez avoir 4 fichiers dans le dossier database : . \u251c\u2500\u2500 pg-credentials.yaml \u251c\u2500\u2500 pg-config.yaml \u251c\u2500\u2500 pg-deployment.yaml \u2514\u2500\u2500 pg-service.yaml Bonus Consulter la base de donn\u00e9es Pour se faire, vous devez r\u00e9aliser les \u00e9tapes suivantes : Utiliser le mode exec pour se connecter \u00e0 Postgres. Lancer la commande psql pour interagir avec la base. Elle devrait avoir la forme suivante : psql -d database -U user Vous pouvez ensuite effectuer les actions suivantes : Lister les bases avec \\d Changer la base de donn\u00e9es actuelle vers cdb-db : \\c cdb-db Lister les tables de la bdd cdb-db avec \\l 3\u1d49 \u00e9tape : Faire pointer l\u2019API sur la base de donn\u00e9es Ressources n\u00e9cessaires Vous disposez maintenant d'une base de donn\u00e9es et d'une API mais vous n\u2019avez pas encore configur\u00e9 le lien entre ces deux services : l\u2019API doit se connecter \u00e0 la base de donn\u00e9es Postgres pour fonctionner. Les variables d\u2019environnements reconnues par l\u2019image de l\u2019API seront utilis\u00e9es : DB_ENDPOINT POSTGRES_DB POSTGRES_USER POSTGRES_PASSWORD L\u2019API est une application qui tourne dans Kubernetes, il est donc possible pour elle d\u2019attaquer la base de donn\u00e9es sur son Service (en interne dans Kubernetes). Pour qu\u2019un Pod puisse joindre un service du m\u00eame Cluster Kubernetes, il doit simplement indiquer le nom du service ainsi que le namespace dans lequel est le service. La nomenclature est la suivante : mon-service.mon-namespace Exemple Une API publi\u00e9e derri\u00e8re un service nomm\u00e9 api dans le namespace \u201ctest\u201d peut \u00eatre requ\u00eat\u00e9e de cette mani\u00e8re : http://api.test Pour la base de donn\u00e9es, le fonctionnement est identique (sans http bien s\u00fbr, car ce n\u2019est pas un serveur Web !). D\u2019ailleurs, quand l'application doit attaquer un service dans le m\u00eame namespace, vous n'\u00eates m\u00eame pas oblig\u00e9 d\u2019indiquer le nom du namespace ! Info Les services se voient attribuer un enregistrement de type DNS A et ont un nom qui a pour forme : mon-service.mon-namespace.svc.cluster.local . La r\u00e9solution de ce nom donne l'adresse ClusterIP du service. Plus d'informations sur l'attribution de DNS pour les services. Question Quel est le nom du service de la base de donn\u00e9es ? Avec ces indications, vous devriez \u00eatre capable de cr\u00e9er un ConfigMap avec la variable DB_ENDPOINT et le POSTGRES_DB (nommez la base de donn\u00e9es \u201ccdb-db\u201d, comme nous l\u2019avons d\u00e9fini \u00e0 la cr\u00e9ation du Postgres en partie 2). Remarquez que les autres variables POSTGRES_USER et POSTGRES_PASSWORD sont d\u00e9j\u00e0 pr\u00e9sentes dans le Secret pg-credentials cr\u00e9\u00e9 dans la partie 2. Vous pourrez donc le r\u00e9utiliser tel quel. \u00c0 vous de jouer Cr\u00e9ez le ConfigMap api-config.yaml avec les clefs DB_ENDPOINT et POSTGRES_DB et d\u00e9ployez-le. Editez votre fichier Deployment de l\u2019API api-deployment.yaml pour utiliser les 4 variables d\u2019environnement (depuis le configMap api-config ET le Secret pg-credentials) et appliquez cette modification. Votre API est maintenant fonctionnelle avec une base de donn\u00e9es ! Requ\u00eatez votre API : http://api.votre_nom.takima.cloud/computers Check Vous devez avoir 1 nouveau fichier dans le dossier api : api-config.yaml Vous devez avoir modifi\u00e9 un fichier existant : api-deployment.yaml 4\u1d49 \u00e9tape : C'est au tour du Front. C\u2019est finalement le plus simple, car le Front n\u2019a pas besoin d\u2019attaquer de service interne. Ressources n\u00e9cessaires L\u2019image Front \u00e0 utiliser est la suivante : master3.takima.io:4567/master3/kubernetes-resources/front:latest Info L'image contient un Nginx qui expose un index.html sur le port 80 . Variables d'environnements utilisables : API_URL Tip Attention, ici c\u2019est l\u2019URL qui sera utilis\u00e9e c\u00f4t\u00e9 navigateur client pour r\u00e9cup\u00e9rer les informations de l\u2019 API . Il faudra donc indiquer l\u2019URL (host) de l\u2019ingress et non pas le nom du service interne Kubernetes de l\u2019 API . \u00c0 vous de jouer Cr\u00e9ez le ConfigMap : front-config.yaml associ\u00e9 \u00e0 la documentation (avec la valeur de l\u2019API URL que vous avez indiqu\u00e9 dans l\u2019Ingress de l\u2019API). Cr\u00e9ez le Deployment : front-deployment.yaml associ\u00e9 \u00e0 cette image. Cr\u00e9ez le Service : front-service.yaml associ\u00e9 \u00e0 ce Deployment. Cr\u00e9ez l\u2019Ingress : front-ingress.yaml pour acc\u00e9der \u00e0 ce service (vous pouvez utiliser front.votre_nom.takima.cloud par exemple) Vous devriez pouvoir acc\u00e9der \u00e0 votre Front : http://front.votre_nom.takima.cloud/ Parfait, vous avez maintenant un applicatif complet pr\u00eat \u00e0 \u00eatre utilis\u00e9. Enfin pr\u00eat\u2026 Pas tout \u00e0 fait ! Il manque quelque chose d\u2019important. Pour le constater : Sur votre navigateur internet, ajoutez un nouveau computer avec le bouton Add . Puis recherchez ce nouveau computer. C\u2019est parfait, rien d\u2019anormal, il est bien l\u00e0. Maintenant, d\u00e9truisez le Pod de la base de donn\u00e9es Postgres (en utilisant Lens ou un kubectl delete pods\u2026). Chouette, le Pod se reconstruit tout seul ! C\u2019est g\u00e9nial, non ? Une fois le Pod Postgres d\u00e9marr\u00e9, retournez sur votre navigateur internet et recherchez \u00e0 nouveau le computer que vous venez de cr\u00e9er... Il a disparu ! Question Pourquoi le computer a disparu ? Check Vous devez avoir 4 nouveaux fichiers dans le dossier front : . \u251c\u2500\u2500 front-config.yaml \u251c\u2500\u2500 front-deployment.yaml \u251c\u2500\u2500 front-service.yaml \u2514\u2500\u2500 front-ingress.yaml 5\u1d49 \u00e9tape : La persistance dans Kubernetes Les containers qui tournent par d\u00e9faut sont donc Stateless. D'ailleurs, la configuration des Pods est immuable : on ne red\u00e9marre pas un Pod, on le d\u00e9truit pour qu\u2019un nouveau Pod r\u00e9apparaisse. Ce qui \u00e0 pour cons\u00e9quence que quand on d\u00e9truit un Pod, toutes ses donn\u00e9es disparaissent avec lui... Ce qu\u2019on appelle la persistance des donn\u00e9es est donc n\u00e9cessaire pour \u00e9viter de perdre les nouveaux computers ajout\u00e9s. Kubernetes apporte ce fonctionnement avec les ressources appel\u00e9es Persistent Volume / Persistent Volume Claim (usuellement PV / PVC). Le Persistent Volume correspond \u00e0 l\u2019abstraction Kubernetes du volume physique mapp\u00e9 sur les serveurs, tandis que le Persistent Volume Claim correspond au mapping d\u2019un Pod ou plusieurs Pods sur ce volume d\u00e9fini. Pour utiliser de la persistance dans Kubernetes et donc cr\u00e9er un PV, il faut au pr\u00e9alable qu\u2019un StorageClass existe et soit consommable. Un StorageClass impl\u00e9mente un provider de stockage. Dans le cas du TP, le StorageClass (et donc le Provider) est d\u00e9j\u00e0 impl\u00e9ment\u00e9 : il s\u2019agit de l\u2019ElasticBlockStore ( EBS ) de type \u201c gp2 \u201d (pour General Purpose) fourni par Amazon Web Service. Il existe une multitude de Providers avec 3 types d\u2019acc\u00e8s diff\u00e9rents dont les noms parlent d\u2019eux-m\u00eame : ReadWriteOnce ReadWriteMany ReadOnlyMany Tableau tr\u00e8s utile pour savoir les types d\u2019acc\u00e8s permis par les Storage Providers. Question D\u2019apr\u00e8s le tableau, quel est le type d\u2019acc\u00e8s impl\u00e9ment\u00e9 par notre Storage Class EBS ? Pourquoi cela convient parfaitement pour la persistance de la base de donn\u00e9es Postgres ? Pour ajouter de la persistance dans un d\u00e9ploiement il faut : Cr\u00e9er la ressources de Type PV / PVC. Utiliser la ressource PVC dans notre Deployment. Note La relation PV - PVC est une relation 1 - 1 et la cr\u00e9ation d\u2019un PVC utilisant un Storage Class cr\u00e9era automatiquement le PV associ\u00e9. \u00c0 vous de jouer Cr\u00e9ez la ressource PVC pg-pvc.yaml et d\u00e9ployez-la : 1 2 3 4 5 6 7 8 9 10 11 12 apiVersion : v1 kind : PersistentVolumeClaim metadata : name : pg-db spec : storageClassName : gp2 accessModes : - ReadWriteOnce volumeMode : Filesystem resources : requests : storage : 3Gi Question V\u00e9rifiez que le PVC est cr\u00e9\u00e9 avec le PV. Quel est le nom du PV ? Le volume est bien instanci\u00e9, mais il n\u2019est pas utilis\u00e9 par un Pod pour le moment. Montez le nouveau PVC dans le Pod Postgres : Pour consommer un PVC dans un Pod, il faut le d\u00e9crire dans la ressource Deployment. Il y a deux ajouts \u00e0 effectuer : La d\u00e9claration du Persistant Volume en tant que volume (d\u00e9finition de son nom). Le montage de ce volume d\u00e9clar\u00e9 dans le Pod (sur un chemin (path) particulier du Container). Voici le block de d\u00e9claration du volume sur le d\u00e9ploiement, \u00e0 placer au niveau spec.template.spec du d\u00e9ploiement : 1 2 3 4 volumes : - name : pg-data persistentVolumeClaim : claimName : pg-db Voici le block de point de montage du volume sur le Pod, \u00e0 placer au niveau spec.template.spec.containers du d\u00e9ploiement : 1 2 3 volumeMounts : - mountPath : /var/lib/postgresql/data name : pg-data Attention Dans le cas pr\u00e9cis d'une base de donn\u00e9es Postgres, une action de plus est n\u00e9cessaire car Postgres n'accepte pas d'avoir un dossier non vide pour s'initialiser, or le storage AWS est livr\u00e9 avec un dossier \"lost/found\". Vous pouvez le constater si vous lancez votre d\u00e9ploiement sans configurer ce qui va suivre : votre base de donn\u00e9es va retourner une erreur. Pour \u00e9viter cela, nous allons installer Postgres dans un sous-chemin de notre point de montage. Notre image Postgres dispose d'une variable d'environement PGDATA qui permet de configurer ce comportement. Modifiez le ConfigMap pg-config.yaml et ajoutez la key:value suivante : 1 db_path : \"/var/lib/postgresql/data/pgdata\" Utilisez ce nouveau param\u00e8tre dans le Deployment de votre base de donn\u00e9es : 1 2 3 4 5 - name : PGDATA valueFrom : configMapKeyRef : name : pg-config # Nom du configmap key : db_path # nom de la clef dans le configMap contenant path ou installer la db dans le volume persistant Check Vous devez avoir 1 nouveau fichier dans le dossier database : pg-pvc.yaml Vous devez avoir modifi\u00e9 deux fichiers existants dans le dossier database : pg-deployment.yaml pg-config.yaml Vous pouvez maintenant essayer de supprimer un ordinateur sur le Front, puis supprimer le pod de la base de donn\u00e9es et constater la persistance des donn\u00e9es. Bonus 1 : Administration de la base de donn\u00e9es Admin de la DB Vous allez maintenant d\u00e9ployer un service permettant d\u2019administrer la base de donn\u00e9es. Il est possible de d\u00e9finir plusieurs ressources Kubernetes dans un unique fichier YAML. Ins\u00e9rez ces ressources dans le fichier database/pgadmin.yaml et appliquez-le. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 apiVersion : apps/v1 kind : Deployment metadata : name : pgadmin labels : app : pgadmin spec : replicas : 1 selector : matchLabels : app : pgadmin template : metadata : labels : app : pgadmin spec : containers : - name : pgadmin image : \"dpage/pgadmin4\" imagePullPolicy : IfNotPresent envFrom : - configMapRef : name : pgadmin env : - name : PGADMIN_DEFAULT_PASSWORD valueFrom : secretKeyRef : name : pgadmin key : pgadmin-password ports : - name : http containerPort : 80 protocol : TCP - name : https containerPort : 443 protocol : TCP --- apiVersion : v1 kind : ConfigMap metadata : name : pgadmin data : PGADMIN_DEFAULT_EMAIL : admin@takima.io --- apiVersion : v1 kind : Secret metadata : name : pgadmin labels : app : pgadmin type : Opaque data : pgadmin-password : \"YWRtaW4xMjMq\" #base64 of admin123* --- apiVersion : v1 kind : Service metadata : name : pgadmin labels : app : pgadmin spec : type : NodePort ports : - port : 80 selector : app : pgadmin La publication du service n'est pas souhait\u00e9e car l\u2019administration d\u2019une base de donn\u00e9es est une action sensible et doit se faire de mani\u00e8re s\u00e9curis\u00e9e. Une fonctionnalit\u00e9 de l\u2019outil Lens permet de monter une connexion distante sur un port local de votre ordinateur. Au niveau Network \u2192 Service , cliquez sur le service pgadmin. Vous pourrez alors lancer le Port Forwarding. Cliquez dessus puis indiquez dans la fen\u00eatre qui s\u2019ouvre un port (par exemple 8081 si ce port n'est pas d\u00e9j\u00e0 utilis\u00e9 sur votre machine) et cochez \"open in browser\" (pas besoin d'utiliser https ici). Cette action devrait ouvrir votre navigateur par d\u00e9faut et la page de pgAdmin. Indiquez le user/password (dans l'exemple du TP : admin@takima.io / admin123*) pgAdmin est une application qui tourne dans Kubernetes, il est donc possible pour elle d\u2019attaquer la base de donn\u00e9es via son Service. Rappel : pour qu\u2019un Pod puisse joindre un service du m\u00eame cluster Kubernetes, il doit simplement indiquer le nom du service ainsi que le Namespace dans lequel est le service. Info Fonctionnement de Service attach\u00e9 \u00e0 un Pod vu \u00e0 l'\u00e9tape 3 Toujours dans pgAdmin : Cliquez droit sur \u201cservers\u201d \u2192 \u201ccreate\u201d \u2192 \u201cserver\u201d Renseignez le host avec \u201cpostres.votre_namespace\u201d Indiquez le user/password Utilisez le bouton Save Vous aurez alors une connexion \u00e0 votre base de donn\u00e9es. Vous retrouverez votre base avec le nom indiqu\u00e9 dans votre ConfigMap. Un initDb est utilis\u00e9 dans l'image et a d\u00fb cr\u00e9er 2 tables dans le sch\u00e9ma public : Computer Company Operation Users Bonus 2 : Les StatefulSets Nous avons cr\u00e9\u00e9 une database avec un deployement associ\u00e9 a un persistant volume. Cela fonctionne, mais quand on essaie de scaler le d\u00e9ploiement, on se retrouve bloqu\u00e9 car, en mode ReadWriteOnce , le volume ne peut pas \u00eatre mont\u00e9 sur plusieurs pods. Kubernetes a pr\u00e9vu une ressource sp\u00e9cialement pour ce genre de d\u00e9ploiement dit Stateful : Les StatefulSets Avec ces ressources, plus besoin de cr\u00e9er au pr\u00e9alable un PVC pour \u00eatre consommer dans le POD. On d\u00e9clare directement ce PVC dans la ressource StatefulSets , et c'est celle-ci qui va piloter la cr\u00e9ation des PVC/PV Mettons cela en pratique pour notre DB postgres. Editez une ressource StatefulSet Le statefulSet fonctionne comme le deployment mais permet d'ajouter un block de provisionning de PVC 1 2 3 4 5 6 7 8 volumeClaimTemplates : - metadata : name : pg-data spec : accessModes : [ \"ReadWriteOnce\" ] resources : requests : storage : 1Gi Ensuite le volume se consomme dans le container de la m\u00eame mani\u00e8re ( block volumeMounts: ) D\u00e9ployez cette ressource v\u00e9rifiez qu'un PVC est bien provisionn\u00e9, et observ\u00e9 son nom ainsi que le nom du pod cr\u00e9\u00e9. Essayez de scaler le statefulSet Attention Ici quand on scale on a des postgres ind\u00e9pendant et pas un cluster Primary/standbye. Ils ne fonctionne pas dans un m\u00eame cluster postgres. Bonus 3 : Operator Postgres Comme vous avez pu le voir, nous pouvons d\u00e9sormais d\u00e9ployer une base de donn\u00e9es Postgres et lui attacher un volume permettant la persistance de la donn\u00e9e. Mais au-del\u00e0 de vouloir sauvegarder la donn\u00e9e, nous aimerions que la base soit accessible tout le temps, m\u00eame si le pod venait \u00e0 tomber. Heureusement pour nous, la communaut\u00e9 Kubernetes a travaill\u00e9 dur pour fournir un Operator Postgres qui peut faire presque tout ce dont vous pouvez r\u00eaver (du moins, pour une base de donn\u00e9es). Nous allons voir \u00e7a ensemble: Deployez une ressource de type postgresql (c\u2019est une CRD ajout\u00e9 par l\u2019operator) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 apiVersion : \"acid.zalan.do/v1\" kind : postgresql metadata : name : formation-cdb spec : teamId : \"formation\" # le team id doit matcher le pr\u00e9fixe dans le metadata.name, ici formation volume : size : 1Gi numberOfInstances : 2 users : cdb : # database owner - superuser - createdb databases : cdb : cdb # dbname: owner postgresql : version : \"14\" Observez les pods se cr\u00e9er : on peut voir le premier pod s\u2019initialiser. Observez les logs de ce premier pod, on doit retrouver \u00e0 plusieurs reprise le message suivant : INFO: no action. I am ( formation-cdb-0 ) the leader with the lock Le deuxi\u00e8me pod (en mode standby) va \u00e9galement se cr\u00e9er dans un second temps (les pods se lancent les uns apr\u00e8s les autres. C\u2019est une particularit\u00e9 des StatefulSet ). Allez voir les logs du pod. On doit retrouver son initialisation sur le master : 2022 -02-02 18 :24:39,452 INFO: trying to bootstrap from leader 'formation-cdb-0' 1024 +0 records in 1024 +0 records out 16777216 bytes ( 17 MB, 16 MiB ) copied, 0 .0123499 s, 1 .4 GB/s NOTICE: all required WAL segments have been archived 2022 -02-02 18 :24:41,013 INFO: replica has been created using basebackup_fast_xlog 2022 -02-02 18 :24:41,014 INFO: bootstrapped from leader 'formation-cdb-0' Puis des logs indiquant qu\u2019il est un r\u00e9plica et qu\u2019il suit le leader: INFO: no action. I am a secondary ( formation-cdb-1 ) and following a leader ( formation-cdb-0 ) Simulons une perte du master D\u00e9truisez le pod faisant tourner le postgres master et pr\u00e9parez vous \u00e0 observer les logs du pod replica. On doit constater qu\u2019il detecte quasi instantan\u00e9ment la perte du noeud primaire et qu\u2019il va faire ce qu\u2019on appelle une promotion pour devenir le nouveau noeud primaire. Apr\u00e8s l\u2019initialisation on doit retrouver dans les logs : INFO: no action. I am ( formation-cdb-1 ) the leader with the lock On a donc bien un cluster avec de la haute disponibilit\u00e9. Pour aller plus loin Supprimez votre ressource PostgreSQL (cela prend un peu de temps avec les StatefulSet . Attendez que les pods disparaissent). Editez ensuite votre ressource yaml postgresql pour y ajouter la configuration suivante : 1 2 3 spec : enableLogicalBackup : true logicalBackupSchedule : 30 00 * * * Puis red\u00e9ployez cette ressource postgres. Vous devez constater qu\u2019une nouvelle ressource kubernetes est pr\u00e9sente : le Cronjob . C\u2019est lui qui d\u00e9clenche les backups logiques : d\u00e9clenchez maintenant le job manuellement. Un pod va \u00eatre provisionn\u00e9, faire un backup du postgres et envoyer tout cela dans un storage S3 (ce n\u2019est pas magique tout a \u00e9t\u00e9 configur\u00e9 dans l\u2019op\u00e9rateur). Demandez \u00e0 un intervenant de voir si le backup a bien \u00e9t\u00e9 r\u00e9ceptionn\u00e9 dans S3. \u00a9 Takima 2022","title":"Day 2 - Deep Dive"},{"location":"ch2-deep-dive/#tp-initiation-kubernetes-j2","text":"","title":"TP initiation Kubernetes (J2)"},{"location":"ch2-deep-dive/#objectifs","text":"Vous savez dor\u00e9navant d\u00e9ployer et administrer un applicatif simple sur Kubernetes. Mais les applicatifs ne se limitent souvent pas qu'\u00e0 un simple Front. Sur les projets, vous aurez par exemple besoin de cr\u00e9er des APIs, des bases de donn\u00e9es, que vos donn\u00e9es soient persistentes, de g\u00e9rer des brokers, etc. L'architecture la plus simple que l'on retrouve dans la majorit\u00e9 des projets est l'architecture 3 tiers. Elle contient les \u00e9l\u00e9ments suivants : - Un front - Une API - Une base de donn\u00e9es Aujourd'hui, l'objectif est de mettre en application ce qui a \u00e9t\u00e9 appris pour d\u00e9ployer un applicatif 3 tiers dans Kubernetes. Rien que \u00e7a ! Etant donn\u00e9 que chaque ressource Kubernetes correspond \u00e0 un fichier yaml, vous allez utiliser une arboresence de dossiers pr\u00e9cise et cr\u00e9er les dossiers suivants pour bien organiser vos fichiers : . \u2514\u2500\u2500 TP-kube-02 \u251c\u2500\u2500 api \u251c\u2500\u2500 database \u2514\u2500\u2500 front Pour la suite du TP, un minimum de ressources concernant chaque service sera fourni et \u00e7e sera \u00e0 vous de cr\u00e9er les ressources Kubernetes n\u00e9cessaires ! Amusez-vous bien !","title":"Objectifs"},{"location":"ch2-deep-dive/#1ere-etape-deployer-lapi","text":"","title":"1\u00e8re \u00e9tape : d\u00e9ployer l\u2019API"},{"location":"ch2-deep-dive/#ressources-necessaires","text":"L\u2019image API \u00e0 utiliser est la suivante : master3.takima.io:4567/master3/kubernetes-resources/api:latest Info L'image contient une API Java Spring Boot et expose ses services sur le port 8080 .","title":"Ressources n\u00e9cessaires"},{"location":"ch2-deep-dive/#a-vous-de-jouer","text":"Cr\u00e9ez le fichier api-deployment.yaml associ\u00e9 \u00e0 cette image et d\u00e9ployez-le. Cr\u00e9ez le fichier api-service.yaml associ\u00e9 au Deployment et d\u00e9ployez-le. Cr\u00e9ez le fichier api-ingress.yaml pour acc\u00e9der \u00e0 ce service et d\u00e9ployez-le. Question Que se passe-t-il au niveau des Pods de l\u2019API ? Vous pouvez jeter un oeil aux logs. ( kubectl logs -f nomdupod ) L\u2019API a donc besoin d\u2019une base de donn\u00e9es pour fonctionner. Vous vous doutez maintenant \u00e0 quoi servent les variables d'environnements utilisables , nous les utiliserons plus tard. Check Vous devez avoir les fichiers suivants dans le dossier api : . \u251c\u2500\u2500 api-deployment.yaml \u251c\u2500\u2500 api-ingress.yaml \u2514\u2500\u2500 api-service.yaml","title":"\u00c0 vous de jouer"},{"location":"ch2-deep-dive/#2e-etape-creer-la-base-de-donnees","text":"Comme l'API a besoin d\u2019une base de donn\u00e9es, il faut lui en fournir une. L\u2019API utilise un SGBDR (Syst\u00e8me de Gestion de Base de Donn\u00e9es Relationnelles) Postgres.","title":"2\u1d49 \u00e9tape : cr\u00e9er la base de donn\u00e9es"},{"location":"ch2-deep-dive/#ressources-necessaires_1","text":"Rappel des variables d'environnements disponibles sur API : DB_ENDPOINT POSTGRES_DB POSTGRES_USER POSTGRES_PASSWORD Image de la base de donn\u00e9es : master3.takima.io:4567/master3/kubernetes-resources/db:latest Info Postgres est accessible depuis le port 5432 . Variables d'environnement utilisables : POSTGRES_PASSWORD POSTGRES_USER POSTGRES_DB","title":"Ressources n\u00e9cessaires"},{"location":"ch2-deep-dive/#a-vous-de-jouer_1","text":"Cr\u00e9ez la ressource de type Secret : pg-credentials.yaml nomm\u00e9 \u201cpg-credentials\u201d contenant le username et le password (choisissez celui que vous souhaitez utiliser mais attention, n\u2019oubliez pas d'encoder les valeurs en base64, comme lors de la fin du tp 1). Cr\u00e9ez un Configmap : pg-config.yaml nomm\u00e9 \u201cpg-config\u201d contenant le nom de la base de donn\u00e9es, nommer la base de donn\u00e9es \u201ccdb-db\u201d. Cr\u00e9ez le Deployment : pg-deployment.yaml associ\u00e9 \u00e0 l'image de la base de donn\u00e9es. Tip Bien penser \u00e0 mapper les variables d'environnement dans le Deployment avec le ConfigMap et le Secret. Penser aux credentials pour pouvoir pull sur le registry priv\u00e9 master3.takima.io. Cr\u00e9ez le Service associ\u00e9 au Deployment : pg-service.yaml (le container \u00e9coute sur le port 5432). Pour v\u00e9rifier que la base de donn\u00e9es fonctionne, vous pouvez afficher les logs et constater qu\u2019elle est bien d\u00e9marr\u00e9e. Check Vous devez avoir 4 fichiers dans le dossier database : . \u251c\u2500\u2500 pg-credentials.yaml \u251c\u2500\u2500 pg-config.yaml \u251c\u2500\u2500 pg-deployment.yaml \u2514\u2500\u2500 pg-service.yaml","title":"\u00c0 vous de jouer"},{"location":"ch2-deep-dive/#bonus","text":"","title":"Bonus"},{"location":"ch2-deep-dive/#consulter-la-base-de-donnees","text":"Pour se faire, vous devez r\u00e9aliser les \u00e9tapes suivantes : Utiliser le mode exec pour se connecter \u00e0 Postgres. Lancer la commande psql pour interagir avec la base. Elle devrait avoir la forme suivante : psql -d database -U user Vous pouvez ensuite effectuer les actions suivantes : Lister les bases avec \\d Changer la base de donn\u00e9es actuelle vers cdb-db : \\c cdb-db Lister les tables de la bdd cdb-db avec \\l","title":"Consulter la base de donn\u00e9es"},{"location":"ch2-deep-dive/#3e-etape-faire-pointer-lapi-sur-la-base-de-donnees","text":"","title":"3\u1d49 \u00e9tape : Faire pointer l\u2019API sur la base de donn\u00e9es"},{"location":"ch2-deep-dive/#ressources-necessaires_2","text":"Vous disposez maintenant d'une base de donn\u00e9es et d'une API mais vous n\u2019avez pas encore configur\u00e9 le lien entre ces deux services : l\u2019API doit se connecter \u00e0 la base de donn\u00e9es Postgres pour fonctionner. Les variables d\u2019environnements reconnues par l\u2019image de l\u2019API seront utilis\u00e9es : DB_ENDPOINT POSTGRES_DB POSTGRES_USER POSTGRES_PASSWORD L\u2019API est une application qui tourne dans Kubernetes, il est donc possible pour elle d\u2019attaquer la base de donn\u00e9es sur son Service (en interne dans Kubernetes). Pour qu\u2019un Pod puisse joindre un service du m\u00eame Cluster Kubernetes, il doit simplement indiquer le nom du service ainsi que le namespace dans lequel est le service. La nomenclature est la suivante : mon-service.mon-namespace Exemple Une API publi\u00e9e derri\u00e8re un service nomm\u00e9 api dans le namespace \u201ctest\u201d peut \u00eatre requ\u00eat\u00e9e de cette mani\u00e8re : http://api.test Pour la base de donn\u00e9es, le fonctionnement est identique (sans http bien s\u00fbr, car ce n\u2019est pas un serveur Web !). D\u2019ailleurs, quand l'application doit attaquer un service dans le m\u00eame namespace, vous n'\u00eates m\u00eame pas oblig\u00e9 d\u2019indiquer le nom du namespace ! Info Les services se voient attribuer un enregistrement de type DNS A et ont un nom qui a pour forme : mon-service.mon-namespace.svc.cluster.local . La r\u00e9solution de ce nom donne l'adresse ClusterIP du service. Plus d'informations sur l'attribution de DNS pour les services. Question Quel est le nom du service de la base de donn\u00e9es ? Avec ces indications, vous devriez \u00eatre capable de cr\u00e9er un ConfigMap avec la variable DB_ENDPOINT et le POSTGRES_DB (nommez la base de donn\u00e9es \u201ccdb-db\u201d, comme nous l\u2019avons d\u00e9fini \u00e0 la cr\u00e9ation du Postgres en partie 2). Remarquez que les autres variables POSTGRES_USER et POSTGRES_PASSWORD sont d\u00e9j\u00e0 pr\u00e9sentes dans le Secret pg-credentials cr\u00e9\u00e9 dans la partie 2. Vous pourrez donc le r\u00e9utiliser tel quel.","title":"Ressources n\u00e9cessaires"},{"location":"ch2-deep-dive/#a-vous-de-jouer_2","text":"Cr\u00e9ez le ConfigMap api-config.yaml avec les clefs DB_ENDPOINT et POSTGRES_DB et d\u00e9ployez-le. Editez votre fichier Deployment de l\u2019API api-deployment.yaml pour utiliser les 4 variables d\u2019environnement (depuis le configMap api-config ET le Secret pg-credentials) et appliquez cette modification. Votre API est maintenant fonctionnelle avec une base de donn\u00e9es ! Requ\u00eatez votre API : http://api.votre_nom.takima.cloud/computers Check Vous devez avoir 1 nouveau fichier dans le dossier api : api-config.yaml Vous devez avoir modifi\u00e9 un fichier existant : api-deployment.yaml","title":"\u00c0 vous de jouer"},{"location":"ch2-deep-dive/#4e-etape-cest-au-tour-du-front","text":"C\u2019est finalement le plus simple, car le Front n\u2019a pas besoin d\u2019attaquer de service interne.","title":"4\u1d49 \u00e9tape : C'est au tour du Front."},{"location":"ch2-deep-dive/#ressources-necessaires_3","text":"L\u2019image Front \u00e0 utiliser est la suivante : master3.takima.io:4567/master3/kubernetes-resources/front:latest Info L'image contient un Nginx qui expose un index.html sur le port 80 . Variables d'environnements utilisables : API_URL Tip Attention, ici c\u2019est l\u2019URL qui sera utilis\u00e9e c\u00f4t\u00e9 navigateur client pour r\u00e9cup\u00e9rer les informations de l\u2019 API . Il faudra donc indiquer l\u2019URL (host) de l\u2019ingress et non pas le nom du service interne Kubernetes de l\u2019 API .","title":"Ressources n\u00e9cessaires"},{"location":"ch2-deep-dive/#a-vous-de-jouer_3","text":"Cr\u00e9ez le ConfigMap : front-config.yaml associ\u00e9 \u00e0 la documentation (avec la valeur de l\u2019API URL que vous avez indiqu\u00e9 dans l\u2019Ingress de l\u2019API). Cr\u00e9ez le Deployment : front-deployment.yaml associ\u00e9 \u00e0 cette image. Cr\u00e9ez le Service : front-service.yaml associ\u00e9 \u00e0 ce Deployment. Cr\u00e9ez l\u2019Ingress : front-ingress.yaml pour acc\u00e9der \u00e0 ce service (vous pouvez utiliser front.votre_nom.takima.cloud par exemple) Vous devriez pouvoir acc\u00e9der \u00e0 votre Front : http://front.votre_nom.takima.cloud/ Parfait, vous avez maintenant un applicatif complet pr\u00eat \u00e0 \u00eatre utilis\u00e9. Enfin pr\u00eat\u2026 Pas tout \u00e0 fait ! Il manque quelque chose d\u2019important. Pour le constater : Sur votre navigateur internet, ajoutez un nouveau computer avec le bouton Add . Puis recherchez ce nouveau computer. C\u2019est parfait, rien d\u2019anormal, il est bien l\u00e0. Maintenant, d\u00e9truisez le Pod de la base de donn\u00e9es Postgres (en utilisant Lens ou un kubectl delete pods\u2026). Chouette, le Pod se reconstruit tout seul ! C\u2019est g\u00e9nial, non ? Une fois le Pod Postgres d\u00e9marr\u00e9, retournez sur votre navigateur internet et recherchez \u00e0 nouveau le computer que vous venez de cr\u00e9er... Il a disparu ! Question Pourquoi le computer a disparu ? Check Vous devez avoir 4 nouveaux fichiers dans le dossier front : . \u251c\u2500\u2500 front-config.yaml \u251c\u2500\u2500 front-deployment.yaml \u251c\u2500\u2500 front-service.yaml \u2514\u2500\u2500 front-ingress.yaml","title":"\u00c0 vous de jouer"},{"location":"ch2-deep-dive/#5e-etape-la-persistance-dans-kubernetes","text":"Les containers qui tournent par d\u00e9faut sont donc Stateless. D'ailleurs, la configuration des Pods est immuable : on ne red\u00e9marre pas un Pod, on le d\u00e9truit pour qu\u2019un nouveau Pod r\u00e9apparaisse. Ce qui \u00e0 pour cons\u00e9quence que quand on d\u00e9truit un Pod, toutes ses donn\u00e9es disparaissent avec lui... Ce qu\u2019on appelle la persistance des donn\u00e9es est donc n\u00e9cessaire pour \u00e9viter de perdre les nouveaux computers ajout\u00e9s. Kubernetes apporte ce fonctionnement avec les ressources appel\u00e9es Persistent Volume / Persistent Volume Claim (usuellement PV / PVC). Le Persistent Volume correspond \u00e0 l\u2019abstraction Kubernetes du volume physique mapp\u00e9 sur les serveurs, tandis que le Persistent Volume Claim correspond au mapping d\u2019un Pod ou plusieurs Pods sur ce volume d\u00e9fini. Pour utiliser de la persistance dans Kubernetes et donc cr\u00e9er un PV, il faut au pr\u00e9alable qu\u2019un StorageClass existe et soit consommable. Un StorageClass impl\u00e9mente un provider de stockage. Dans le cas du TP, le StorageClass (et donc le Provider) est d\u00e9j\u00e0 impl\u00e9ment\u00e9 : il s\u2019agit de l\u2019ElasticBlockStore ( EBS ) de type \u201c gp2 \u201d (pour General Purpose) fourni par Amazon Web Service. Il existe une multitude de Providers avec 3 types d\u2019acc\u00e8s diff\u00e9rents dont les noms parlent d\u2019eux-m\u00eame : ReadWriteOnce ReadWriteMany ReadOnlyMany Tableau tr\u00e8s utile pour savoir les types d\u2019acc\u00e8s permis par les Storage Providers. Question D\u2019apr\u00e8s le tableau, quel est le type d\u2019acc\u00e8s impl\u00e9ment\u00e9 par notre Storage Class EBS ? Pourquoi cela convient parfaitement pour la persistance de la base de donn\u00e9es Postgres ? Pour ajouter de la persistance dans un d\u00e9ploiement il faut : Cr\u00e9er la ressources de Type PV / PVC. Utiliser la ressource PVC dans notre Deployment. Note La relation PV - PVC est une relation 1 - 1 et la cr\u00e9ation d\u2019un PVC utilisant un Storage Class cr\u00e9era automatiquement le PV associ\u00e9.","title":"5\u1d49 \u00e9tape : La persistance dans Kubernetes"},{"location":"ch2-deep-dive/#a-vous-de-jouer_4","text":"Cr\u00e9ez la ressource PVC pg-pvc.yaml et d\u00e9ployez-la : 1 2 3 4 5 6 7 8 9 10 11 12 apiVersion : v1 kind : PersistentVolumeClaim metadata : name : pg-db spec : storageClassName : gp2 accessModes : - ReadWriteOnce volumeMode : Filesystem resources : requests : storage : 3Gi Question V\u00e9rifiez que le PVC est cr\u00e9\u00e9 avec le PV. Quel est le nom du PV ? Le volume est bien instanci\u00e9, mais il n\u2019est pas utilis\u00e9 par un Pod pour le moment. Montez le nouveau PVC dans le Pod Postgres : Pour consommer un PVC dans un Pod, il faut le d\u00e9crire dans la ressource Deployment. Il y a deux ajouts \u00e0 effectuer : La d\u00e9claration du Persistant Volume en tant que volume (d\u00e9finition de son nom). Le montage de ce volume d\u00e9clar\u00e9 dans le Pod (sur un chemin (path) particulier du Container). Voici le block de d\u00e9claration du volume sur le d\u00e9ploiement, \u00e0 placer au niveau spec.template.spec du d\u00e9ploiement : 1 2 3 4 volumes : - name : pg-data persistentVolumeClaim : claimName : pg-db Voici le block de point de montage du volume sur le Pod, \u00e0 placer au niveau spec.template.spec.containers du d\u00e9ploiement : 1 2 3 volumeMounts : - mountPath : /var/lib/postgresql/data name : pg-data Attention Dans le cas pr\u00e9cis d'une base de donn\u00e9es Postgres, une action de plus est n\u00e9cessaire car Postgres n'accepte pas d'avoir un dossier non vide pour s'initialiser, or le storage AWS est livr\u00e9 avec un dossier \"lost/found\". Vous pouvez le constater si vous lancez votre d\u00e9ploiement sans configurer ce qui va suivre : votre base de donn\u00e9es va retourner une erreur. Pour \u00e9viter cela, nous allons installer Postgres dans un sous-chemin de notre point de montage. Notre image Postgres dispose d'une variable d'environement PGDATA qui permet de configurer ce comportement. Modifiez le ConfigMap pg-config.yaml et ajoutez la key:value suivante : 1 db_path : \"/var/lib/postgresql/data/pgdata\" Utilisez ce nouveau param\u00e8tre dans le Deployment de votre base de donn\u00e9es : 1 2 3 4 5 - name : PGDATA valueFrom : configMapKeyRef : name : pg-config # Nom du configmap key : db_path # nom de la clef dans le configMap contenant path ou installer la db dans le volume persistant Check Vous devez avoir 1 nouveau fichier dans le dossier database : pg-pvc.yaml Vous devez avoir modifi\u00e9 deux fichiers existants dans le dossier database : pg-deployment.yaml pg-config.yaml Vous pouvez maintenant essayer de supprimer un ordinateur sur le Front, puis supprimer le pod de la base de donn\u00e9es et constater la persistance des donn\u00e9es.","title":"\u00c0 vous de jouer"},{"location":"ch2-deep-dive/#bonus-1-administration-de-la-base-de-donnees","text":"","title":"Bonus 1 : Administration de la base de donn\u00e9es"},{"location":"ch2-deep-dive/#admin-de-la-db","text":"Vous allez maintenant d\u00e9ployer un service permettant d\u2019administrer la base de donn\u00e9es. Il est possible de d\u00e9finir plusieurs ressources Kubernetes dans un unique fichier YAML. Ins\u00e9rez ces ressources dans le fichier database/pgadmin.yaml et appliquez-le. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 apiVersion : apps/v1 kind : Deployment metadata : name : pgadmin labels : app : pgadmin spec : replicas : 1 selector : matchLabels : app : pgadmin template : metadata : labels : app : pgadmin spec : containers : - name : pgadmin image : \"dpage/pgadmin4\" imagePullPolicy : IfNotPresent envFrom : - configMapRef : name : pgadmin env : - name : PGADMIN_DEFAULT_PASSWORD valueFrom : secretKeyRef : name : pgadmin key : pgadmin-password ports : - name : http containerPort : 80 protocol : TCP - name : https containerPort : 443 protocol : TCP --- apiVersion : v1 kind : ConfigMap metadata : name : pgadmin data : PGADMIN_DEFAULT_EMAIL : admin@takima.io --- apiVersion : v1 kind : Secret metadata : name : pgadmin labels : app : pgadmin type : Opaque data : pgadmin-password : \"YWRtaW4xMjMq\" #base64 of admin123* --- apiVersion : v1 kind : Service metadata : name : pgadmin labels : app : pgadmin spec : type : NodePort ports : - port : 80 selector : app : pgadmin La publication du service n'est pas souhait\u00e9e car l\u2019administration d\u2019une base de donn\u00e9es est une action sensible et doit se faire de mani\u00e8re s\u00e9curis\u00e9e. Une fonctionnalit\u00e9 de l\u2019outil Lens permet de monter une connexion distante sur un port local de votre ordinateur. Au niveau Network \u2192 Service , cliquez sur le service pgadmin. Vous pourrez alors lancer le Port Forwarding. Cliquez dessus puis indiquez dans la fen\u00eatre qui s\u2019ouvre un port (par exemple 8081 si ce port n'est pas d\u00e9j\u00e0 utilis\u00e9 sur votre machine) et cochez \"open in browser\" (pas besoin d'utiliser https ici). Cette action devrait ouvrir votre navigateur par d\u00e9faut et la page de pgAdmin. Indiquez le user/password (dans l'exemple du TP : admin@takima.io / admin123*) pgAdmin est une application qui tourne dans Kubernetes, il est donc possible pour elle d\u2019attaquer la base de donn\u00e9es via son Service. Rappel : pour qu\u2019un Pod puisse joindre un service du m\u00eame cluster Kubernetes, il doit simplement indiquer le nom du service ainsi que le Namespace dans lequel est le service. Info Fonctionnement de Service attach\u00e9 \u00e0 un Pod vu \u00e0 l'\u00e9tape 3 Toujours dans pgAdmin : Cliquez droit sur \u201cservers\u201d \u2192 \u201ccreate\u201d \u2192 \u201cserver\u201d Renseignez le host avec \u201cpostres.votre_namespace\u201d Indiquez le user/password Utilisez le bouton Save Vous aurez alors une connexion \u00e0 votre base de donn\u00e9es. Vous retrouverez votre base avec le nom indiqu\u00e9 dans votre ConfigMap. Un initDb est utilis\u00e9 dans l'image et a d\u00fb cr\u00e9er 2 tables dans le sch\u00e9ma public : Computer Company Operation Users","title":"Admin de la DB"},{"location":"ch2-deep-dive/#bonus-2-les-statefulsets","text":"Nous avons cr\u00e9\u00e9 une database avec un deployement associ\u00e9 a un persistant volume. Cela fonctionne, mais quand on essaie de scaler le d\u00e9ploiement, on se retrouve bloqu\u00e9 car, en mode ReadWriteOnce , le volume ne peut pas \u00eatre mont\u00e9 sur plusieurs pods. Kubernetes a pr\u00e9vu une ressource sp\u00e9cialement pour ce genre de d\u00e9ploiement dit Stateful : Les StatefulSets Avec ces ressources, plus besoin de cr\u00e9er au pr\u00e9alable un PVC pour \u00eatre consommer dans le POD. On d\u00e9clare directement ce PVC dans la ressource StatefulSets , et c'est celle-ci qui va piloter la cr\u00e9ation des PVC/PV Mettons cela en pratique pour notre DB postgres. Editez une ressource StatefulSet Le statefulSet fonctionne comme le deployment mais permet d'ajouter un block de provisionning de PVC 1 2 3 4 5 6 7 8 volumeClaimTemplates : - metadata : name : pg-data spec : accessModes : [ \"ReadWriteOnce\" ] resources : requests : storage : 1Gi Ensuite le volume se consomme dans le container de la m\u00eame mani\u00e8re ( block volumeMounts: ) D\u00e9ployez cette ressource v\u00e9rifiez qu'un PVC est bien provisionn\u00e9, et observ\u00e9 son nom ainsi que le nom du pod cr\u00e9\u00e9. Essayez de scaler le statefulSet Attention Ici quand on scale on a des postgres ind\u00e9pendant et pas un cluster Primary/standbye. Ils ne fonctionne pas dans un m\u00eame cluster postgres.","title":"Bonus 2 : Les StatefulSets"},{"location":"ch2-deep-dive/#bonus-3-operator-postgres","text":"Comme vous avez pu le voir, nous pouvons d\u00e9sormais d\u00e9ployer une base de donn\u00e9es Postgres et lui attacher un volume permettant la persistance de la donn\u00e9e. Mais au-del\u00e0 de vouloir sauvegarder la donn\u00e9e, nous aimerions que la base soit accessible tout le temps, m\u00eame si le pod venait \u00e0 tomber. Heureusement pour nous, la communaut\u00e9 Kubernetes a travaill\u00e9 dur pour fournir un Operator Postgres qui peut faire presque tout ce dont vous pouvez r\u00eaver (du moins, pour une base de donn\u00e9es). Nous allons voir \u00e7a ensemble:","title":"Bonus 3 : Operator Postgres"},{"location":"ch2-deep-dive/#deployez-une-ressource-de-type-postgresql-cest-une-crd-ajoute-par-loperator","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 apiVersion : \"acid.zalan.do/v1\" kind : postgresql metadata : name : formation-cdb spec : teamId : \"formation\" # le team id doit matcher le pr\u00e9fixe dans le metadata.name, ici formation volume : size : 1Gi numberOfInstances : 2 users : cdb : # database owner - superuser - createdb databases : cdb : cdb # dbname: owner postgresql : version : \"14\" Observez les pods se cr\u00e9er : on peut voir le premier pod s\u2019initialiser. Observez les logs de ce premier pod, on doit retrouver \u00e0 plusieurs reprise le message suivant : INFO: no action. I am ( formation-cdb-0 ) the leader with the lock Le deuxi\u00e8me pod (en mode standby) va \u00e9galement se cr\u00e9er dans un second temps (les pods se lancent les uns apr\u00e8s les autres. C\u2019est une particularit\u00e9 des StatefulSet ). Allez voir les logs du pod. On doit retrouver son initialisation sur le master : 2022 -02-02 18 :24:39,452 INFO: trying to bootstrap from leader 'formation-cdb-0' 1024 +0 records in 1024 +0 records out 16777216 bytes ( 17 MB, 16 MiB ) copied, 0 .0123499 s, 1 .4 GB/s NOTICE: all required WAL segments have been archived 2022 -02-02 18 :24:41,013 INFO: replica has been created using basebackup_fast_xlog 2022 -02-02 18 :24:41,014 INFO: bootstrapped from leader 'formation-cdb-0' Puis des logs indiquant qu\u2019il est un r\u00e9plica et qu\u2019il suit le leader: INFO: no action. I am a secondary ( formation-cdb-1 ) and following a leader ( formation-cdb-0 )","title":"Deployez une ressource de type postgresql (c\u2019est une CRD ajout\u00e9 par l\u2019operator)"},{"location":"ch2-deep-dive/#simulons-une-perte-du-master","text":"D\u00e9truisez le pod faisant tourner le postgres master et pr\u00e9parez vous \u00e0 observer les logs du pod replica. On doit constater qu\u2019il detecte quasi instantan\u00e9ment la perte du noeud primaire et qu\u2019il va faire ce qu\u2019on appelle une promotion pour devenir le nouveau noeud primaire. Apr\u00e8s l\u2019initialisation on doit retrouver dans les logs : INFO: no action. I am ( formation-cdb-1 ) the leader with the lock On a donc bien un cluster avec de la haute disponibilit\u00e9.","title":"Simulons une perte du master"},{"location":"ch2-deep-dive/#pour-aller-plus-loin","text":"Supprimez votre ressource PostgreSQL (cela prend un peu de temps avec les StatefulSet . Attendez que les pods disparaissent). Editez ensuite votre ressource yaml postgresql pour y ajouter la configuration suivante : 1 2 3 spec : enableLogicalBackup : true logicalBackupSchedule : 30 00 * * * Puis red\u00e9ployez cette ressource postgres. Vous devez constater qu\u2019une nouvelle ressource kubernetes est pr\u00e9sente : le Cronjob . C\u2019est lui qui d\u00e9clenche les backups logiques : d\u00e9clenchez maintenant le job manuellement. Un pod va \u00eatre provisionn\u00e9, faire un backup du postgres et envoyer tout cela dans un storage S3 (ce n\u2019est pas magique tout a \u00e9t\u00e9 configur\u00e9 dans l\u2019op\u00e9rateur). Demandez \u00e0 un intervenant de voir si le backup a bien \u00e9t\u00e9 r\u00e9ceptionn\u00e9 dans S3. \u00a9 Takima 2022","title":"Pour aller plus loin"},{"location":"ch3-gitops/","text":"TP initiation Kubernetes (J3) Objectifs Vous poss\u00e9dez maintenant toutes les ressources yaml qui d\u00e9crivent votre applicatif et vous pouvez donc d\u00e9ployer rapidement toute votre stack dans Kubernetes. C\u2019est un bon d\u00e9but ! Gr\u00e2ce \u00e0 cela, vous pouvez d\u00e9truire et construire un environnement complet facilement. De plus, toutes les probl\u00e9matiques d'exploitation sont g\u00e9r\u00e9es par Kubernetes (High Availability, Scaling, Auto Healing, Loadbalancing, etc.). Avant d'aller un peu plus loin dans la partie d\u00e9ploiement, un petit tour parmi le monde merveilleux des Operators Kubernetes s'impose ! Initiation aux Operators Kubernetes Les Operators Kubernetes permettent d'enrichir Kubernetes avec de nouvelles ressources personalis\u00e9es appel\u00e9es CRD (Custom Ressource Definition). Il en existe pour une multitude de produits et vous pouvez m\u00eame cr\u00e9er votre propre CRD si l'envie vous prend (si c'est utile bien s\u00fbr) ! Le cas de l'Operator Elasticsearch, appel\u00e9 ECK pour Elastic Cloud on Kubernetes , sera \u00e9tudi\u00e9 pour cette mise en pratique. ECK ECK est un Operator qui ajoute la possibilit\u00e9 de cr\u00e9er plusieurs CRDs, notamment : Elasticsearch Kibana Les noms sont assez explicites : Elasticsearch d\u00e9ploie un cluster Elasticsearch, et Kibana le dashboard de la stack ELK. Comme les Operators s'installent au niveau global du Cluster Kubernetes (un Operator ECK par Cluster), l'installation se fera en mode D\u00e9mo . Demo Voil\u00e0 l'Operator est install\u00e9, c'est maintenant \u00e0 vous de d\u00e9ployer une Stack Elastic ! La ressource Elasticsearch Tout d'abord, cr\u00e9ez un dossier elk dans lequel vous \u00e9diterez les ressources YAML. Vous allez ensuite commencer par le plus important : la ressource elasticsearch . Editez un yaml elasticsearch.yaml . Quelqu'un s'est amus\u00e9 \u00e0 effacer des valeurs dans le yaml !! 1 2 3 4 5 6 7 8 9 10 11 apiVersion : elasticsearch.k8s.elastic.co/v1 kind : metadata : name : spec : version : 7.16.0 nodeSets : - name : default count : 1 config : node.store.allow_mmap : false 2. V\u00e9rifiez le bon d\u00e9ploiement d'Elasticsearch. #commandes utiles: kubectl get elasticsearch kubectl describe elasticsearch name 3. Observez quelles ressources natives ce CRD a cr\u00e9\u00e9. La ressource Kibana Que serait un cluster Elasticsearch sans son fid\u00e8le dashboard Kibana ? Cr\u00e9ez un fichier YAML kibana.yaml dans le dossier elk Par contre un petit bug est pass\u00e9 par l\u00e0, plusieurs lignes ont disparu !! Vous devez corriger ce qu'il manque dans elasticsearchRef 1 2 3 4 5 6 7 8 9 apiVersion : kibana.k8s.elastic.co/v1 kind : metadata : name : spec : version : count : 1 elasticsearchRef : name : 2. Cr\u00e9ez un Ingress pour pouvoir acc\u00e9der \u00e0 Kibana depuis le navigateur internet kibana-ingress.yaml . Pour l'Ingress, vous devriez y arriver avec tout ce que vous avez d\u00e9j\u00e0 appris. Tip Regarder les services que ES propose et les ports associ\u00e9s Attention le pod Kibana est en TLS il faut donc rajouter ces annotations dans votre ingress: nginx.ingress.kubernetes.io/backend-protocol: HTTPS Acc\u00e9dez au Kibana avec l'URL indiqu\u00e9e dans l'Ingress. Tip Pour l'authentification, observez le Secret -es-elastic-user qui contient le mot de passe et qui a \u00e9t\u00e9 cr\u00e9\u00e9 par le CRD Elasticsearch. L'utilisateur est elastic . Une fois connect\u00e9, Kibana vous propose de d\u00e9couvrir le produit, mais pas de temps \u00e0 perdre : cliquez directement sur Explore my own ! V\u00e9rifiez que Kibana est bien li\u00e9 au Cluster Elasticsearch. Pour se faire : Naviguez sur le menu en haut \u00e0 gauche. Cliquez sur Dev tools. Il est maintenant possible de requ\u00eater le cluster : Pour voir son \u00e9tat de sant\u00e9 et les nodes du cluster : GET _cat/health?v GET _cat/allocation?v Vous pouvez retrouver la taille du volume provisionn\u00e9. Aller plus loin avec ELK Scalez le Cluster ELK \u00e0 3 noeuds : \u00e9ditez le CRD Elasticsearch et observez les nouveaux Pods elasticsearch se cr\u00e9er et s'initialiser un par un. Retournez dans Kibana et relancer : GET _cat/health?v GET _cat/allocation?v Question Que constatez-vous ? V\u00e9rifiez que le Cluster fonctionne correctement. # Cr\u00e9\u00e9r l'index tp et ajouter un doc PUT tp/_doc/1 { \"body\": \"hello\" } # requ\u00eater le doc GET tp/_doc/1 # V\u00e9rifier que l'index tp existe bien et qu'il contient un seul doc GET _cat/indices?v Cet exemple assez simple permet de d\u00e9montrer qu'il est facile de cr\u00e9er des stacks Elasticsearch rapidement et \u00e0 la demande. Bien s\u00fbr, la configuration du CRD Elasticsearch ou Kibana peut aller bien plus loin (aussi loin que la config Elasticsearch classique le permet). Check Le dossier elk doit contenir les \u00e9l\u00e9ments suivants : . \u251c\u2500\u2500 elasticsearch.yaml \u251c\u2500\u2500 kibana.yaml \u2514\u2500\u2500 kibana-ingress.yaml Helm Apr\u00e8s avoir fait un tour sur les Operator, retour aux ressources pr\u00e9c\u00e9dentes : l'API, le Front et la base de donn\u00e9es. Vous vous demandez peut \u00eatre comment g\u00e9rer plusieurs environnements, des variables entre les ressources YAML, des conditions en fonction de l'environnement, etc. Aujourd'hui ce n'est pas possible, n'est-ce-pas ? Surtout que dans cette formation vous utilisez un petit applicatif mais imaginez que vous en ayez des centaines \u00e0 g\u00e9rer (avec les architectures microservices, \u00e7a peut arriver vite !) : cela deviendrait rapidement ing\u00e9rable. Le constat est l\u00e0 : les ressources actuelles sont assez statiques en l\u2019\u00e9tat. Il est possible d'industrialiser tout cela. Installation Dans un premier temps, vous allez installer Helm, ArgoCD & Git. Linux - Package manager - Debian/Ubuntu curl https://baltocdn.com/helm/signing.asc | sudo apt-key add - sudo apt-get install apt-transport-https --yes echo \"deb https://baltocdn.com/helm/stable/debian/ all main\" | sudo tee /etc/apt/sources.list.d/helm-stable-debian.list sudo apt-get update sudo apt-get install helm # git add-apt-repository ppa:git-core/ppa apt update apt install git Linux - Script curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 chmod 700 get_helm.sh ./get_helm.sh # must works fine helm version # git # utiliser un gestionnaire de packet ou un t\u00e9l\u00e9chargement direct : https://git-scm.com/book/fr/v2/D%C3%A9marrage-rapide-Installation-de-Git MacOS brew install helm # must works fine helm version # Git https://git-scm.com/download/mac Windows # Helm https://helm.sh/docs/intro/install/ # ArgoCD https://github.com/argoproj/argo-cd/releases/download/v2.2.3/argocd-windows-amd64.exe # Git http://git-scm.com/download/win Faire des templates avec Helm Pr\u00e9ambule Helm est un outil indispensable pour d\u00e9ployer des stacks applicatives compl\u00e8tes dans Kubernetes. Pratique, c\u2019est un projet maintenu par la CNCF . L\u2019id\u00e9e est simple : permettre de faire des templates de toutes les ressources yaml d\u2019une application donn\u00e9e et les d\u00e9ployer sur Kubernetes ! En d\u00e9finitive, cela pourrait s'apparenter \u00e0 la possibli\u00e9 de d\u00e9ployer des plateformes compl\u00e8tes, clef en main, un peu sur le mod\u00e8le PaaS, Plateforme as a Service. Une stack applicative qui se d\u00e9ploie via Helm se nomme un Chart Helm . L\u2019objectif ici est donc de cr\u00e9er un Chart Helm pour votre stack ComputerDatabase. Setup Avant d'utiliser Helm, vous allez mettre en place un d\u00e9p\u00f4t Git avec les ressources Kubernetes fournises pour ce TP. Cela permettra plus tard d'y connecter ArgoCD. Vous pouvez passer directement \u00e0 l'\u00e9tape 3 si vous avez d\u00e9j\u00e0 un compte Git configur\u00e9 sur Gitlab.com. Cr\u00e9er un compte sur https://gitlab.com/ . Configurez un mot de passe \u00e0 votre compte si vous vous \u00eates connect\u00e9 sans \u00e9diter de mot de passe page suivante . Cr\u00e9ez un repository sur Gitlab https://gitlab.com/projects/new#blank_project . Commandes \u00e0 lancer une fois votre repository cr\u00e9\u00e9 : git clone https://gitlab.com/votreUser/ch3_gitops.git # votre username & password de gitlab seront demand\u00e9s. cd ch3_gitops T\u00e9l\u00e9chargez les premi\u00e8res ressources pour Helm. D\u00e9compressez le contenu de l'archive et d\u00e9placez le contenu du dossier k8s-trainees-main/boilerplate/day-3/step-1 dans votre repository Git ch3_gitops . Une fois les ressources t\u00e9l\u00e9charg\u00e9es, faites un premier commit et push ! git commit -am \"Init Helm resources\" git push origin main Une fois l'\u00e9tape de commit r\u00e9alis\u00e9e vous pouvez jeter un coup d'oeil aux diff\u00e9rents \u00e9l\u00e9ments fournis : templates/ : contient une version template de toutes les ressources Kubernetes que vous devez d\u00e9ployer. Chart.yaml : d\u00e9crit votre chart (la version, le nom, les mainteners). values.yaml : contient toutes les variables qui seront utilis\u00e9es dans les ressources. D\u2019une certaine mani\u00e8re, il d\u00e9crit de mani\u00e8re centrale l'infra \u00e0 d\u00e9ployer. Regardez d'un peu plus pr\u00e8s un Template, le Service du Front : templates/front-service.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 {{ - if .Values.front.enabled }} apiVersion : v1 kind : Service metadata : name : {{ .Values.front.serviceName }} spec : selector : app : front ports : - protocol : TCP port : {{ .Values.front.servicePort }} targetPort : 80 {{ end }} L'usage de variables Helm est soulign\u00e9 sur les deux lignes en jaune (lignes 5 & 11). Chaque variable fait r\u00e9f\u00e9rence \u00e0 une valeur dans le fichier values.yaml : values.yaml 22 23 24 25 26 27 28 29 30 ... front : enabled : true image : repository : master3.takima.io:4567/master3/kubernetes-resources/front tag : latest serviceName : 'front' servicePort : 80 ... Info Il est donc possible de facilement variabiliser des \u00e9l\u00e9ments en pla\u00e7ant les valeurs dans les fichiers values.yaml et y faisant r\u00e9f\u00e9rence via la syntaxe {{ .Values.x.y }} . Info Par la m\u00eame occasion, vous remarquerez que la premi\u00e8re ligne est une condition sur la variable .Values.front.enabled qui englobe l'ensemble du fichier. Ce qui se traduit par : si le Front est enable tout le fichier sera g\u00e9n\u00e9r\u00e9 et cette ressource sera donc d\u00e9ploy\u00e9e. templates/front-service.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 {{ - if .Values.front.enabled }} apiVersion : v1 kind : Service metadata : name : {{ .Values.front.serviceName }} spec : selector : app : front ports : - protocol : TCP port : {{ .Values.front.servicePort }} targetPort : 80 {{ end }} Vous pouvez ensuite tester Helm pour g\u00e9n\u00e9rer vos ressources \u00e0 partir de vos Templates et Ressources : helm template --values ./values.yaml ./ --output-dir dist Vous retrouverez les ressources Kubernetes g\u00e9n\u00e9r\u00e9es avec vos valeurs dans le dossier dist ! Tip Vous pouvez aussi utiliser la commande suivante pour debug votre Chart sans cr\u00e9er de fichiers : helm install cdb --dry-run --debug ./ Vous pouvez tester de d\u00e9ployer l'ensemble de votre Chart sur votre cluster avec la commande suivante : helm install cdb ./ Info La commande helm install|upgrade n'utilse pas les fichiers g\u00e9n\u00e9r\u00e9s dans le dossier dist , elle utilise directement vos sources. Help Vous aurez certainement besoin de configurer le Secret (avec les identifiants re\u00e7us par mail lors du premier jour) pour pull les images : kubectl create secret docker-registry auth-master3-registry --docker-server = master3.takima.io:4567 --docker-username = <USERNAME> --docker-password = <PASSWORD> \u00c0 vous de jouer ! Maintenant que votre \u00e9quipe vous a fourni le Front , \u00e0 vous de faire la m\u00eame chose pour l' API et la DB ! Info Rappel des ressources \u00e0 g\u00e9n\u00e9rer : 1. api-deployement.yaml 2. api-ingress.yaml 3. api-service.yaml 4. api-config.yaml 5. pg-credentials.yaml 6. pg-deployment.yaml 7. pg-pvc.yaml 8. pg-service.yaml Help Vous pouvez reprendre les ressources cr\u00e9\u00e9es dans la partie pr\u00e9c\u00e9dente pour vous aider : https://gitlab.takima.io/school/k8s-trainees/-/archive/main/k8s-trainees-main.tar.gz?path=boilerplate/day-2/step-5 Tip Pensez \u00e0 bien utiliser le syst\u00e8me de templating avec le fichier values.yaml et les diff\u00e9rents outils de Helm . Une fois termin\u00e9, vous pouvez mettre \u00e0 jour votre Chart sur le Cluster avec la commande : helm upgrade cdb ./ Bonus 1 Et si je vous demandais de changer le nom de votre application ? Oui, quand on fait du templating, on remarque aussi que parfois, il y a beaucoup de \"redite\". Les labels, le nom de l'application, etc... C'est pour \u00e7a que helm vous permet de cr\u00e9er des variables suppl\u00e9mentaires. Par convention, nous recommandons de cr\u00e9er un fichier templates/_helpers.tpl . De mani\u00e8re traditionnelle, le fichier peut contenir quelques lignes comme \u00e7a : {{ /* Expand the name of the chart. */ }} {{ - define \"MyAppCtx.name\" - }} {{ - default .Chart.Name | trunc 63 | trimSuffix \"-\" }} {{ - end }} {{ /* Application image tag We select by default the Chart appVersion or an override in values */ }} {{ - define \"MyAppCtx.imageTag\" }} {{ - $name : = default .Chart.AppVersion .Values.image.tag }} {{ - printf \"%s\" $name }} {{ - end }} {{ /* Create a default fully qualified app name. We truncate at 63 chars because some Kubernetes name fields are limited to this ( by the DNS naming spec ) . */ }} {{ - define \"MyAppCtx.fullname\" }} {{ - $name : = default .Chart.Name .Values.nameOverride - }} {{ - printf \"%s-%s\" .Release.Name $name | trunc 63 | trimSuffix \"-\" }} {{ - end }} {{ /* Create chart name and version as used by the chart label. */ }} {{ - define \"MyAppCtx.chart\" - }} {{ - printf \"%s-%s\" .Chart.Name .Chart.Version | replace \"+\" \"_\" | trunc 63 | trimSuffix \"-\" }} {{ - end }} {{ /* Common labels */ }} {{ - define \"MyAppCtx.labels\" - }} helm.sh/chart: {{ include \"MyAppCtx.chart\" . }} {{ include \"MyAppCtx.selectorLabels\" . }} {{ - if .Chart.AppVersion }} app.kubernetes.io/version: {{ .Chart.AppVersion | quote }} {{ - end }} app.kubernetes.io/managed-by: {{ .Release.Service }} {{ - end }} {{ /* Selector labels */ }} {{ - define \"MyAppCtx.selectorLabels\" - }} app.kubernetes.io/name: {{ include \"MyAppCtx.name\" . }} app.kubernetes.io/instance: {{ .Release.Name }} {{ - end }} Ces variables pourront \u00eatre utilis\u00e9es dans votre chart. Appliquez les variables qui vous semblent importantes dans l'ensemble de vos ressources. Objectifs : * une modification du chart name ou du nameOverride dans les values doit modifier le nom de toutes vos ressources. * vous ne devriez plus avoir besoin de serviceName ou tls secretName dans vos values.yaml. Faites bien attention \u00e0 vos selectors et matchSelectors ! Bonus 2 Peut-\u00eatre avez-vous remarqu\u00e9 qu'une modification de ConfigMap ne red\u00e9marrait pas les deployments. C'est normal, car vos applications sont cens\u00e9es pouvoir consommer les ConfigMaps d\u00e8s qu'elles le souhaitent. Pour forcer un red\u00e9ploiement lors de la modification d'une ConfigMap associ\u00e9e, vous pouvez d\u00e9clarer une annotation suppl\u00e9mentaire dans le Deployment concern\u00e9 : annotations : - checksum/config : {{ include (print $.Template.BasePath \"/configmap.yaml\") . | sha256sum }} D\u00e8s que le hash de la configmap changera, le deployment sera consid\u00e9r\u00e9 comme diff\u00e9rent, et donc appliqu\u00e9. Bonus 2 Maintenant, votre \u00e9quipe revient vers vous et souhaite ajouter un nouvel environnement de test. Aucun probl\u00e8me car vous avez d\u00e9j\u00e0 toute la structure avec Helm template ! L'environnement de test se pr\u00e9sente comme suit : API_URL = api.staging.votreusername.takima.cloud WWW_URL = www.staging.votreusername.takima.cloud Tip Vous pouvez ajouter autant de Values que vous le souhaitez avec la commande Helm : helm template --values ./values.yaml --values ./values.env.yaml ./ --output-dir dist \u00c0 vous de jouer Avec toutes les informations dont vous diposez, vous pouvez cr\u00e9er les fichiers n\u00e9cessaires pour que votre Chart s'adapte facilement avec le nouvel environnement. GitOps avec ArgoCD Avant de commencer cette partie, pensez \u00e0 d\u00e9truire l'environnement cr\u00e9\u00e9 avec vos templates Helm : helm delete mon-app Vous savez faire un joli Template Helm que vous d\u00e9ployez \u00e0 la main. La premi\u00e8re chose \u00e0 dire, c'est qu'il existe de multiples mani\u00e8res de faire du GitOps : il serait facile de mettre en oeuvre un peu de scripting avec Gitlab-CI par exemple, pour faire un helm upgrade... automatiquement lorsqu'on veut d\u00e9ployer un applicatif sur un cluster. L'inconv\u00e9nient avec cette approche est que le flux serait uniquement descendant (de Gitlab au Cluster). Argo CD a deux r\u00f4les majeurs : Faciliter la configuration de ces jobs (plus facile que via du scripting). Synchroniser en permanence l'\u00e9tat du Cluster avec celui de Git, m\u00eame si les ressources du Cluster changent de leur c\u00f4t\u00e9. Vous aurez l'occasion de v\u00e9rifier et constater cela dans le TP ! Acc\u00e9dez au portail ArgoCD. Rendez-vous sur https://argocd.takima.cloud et entrez votre username / mot de passe (cf mail envoy\u00e9). Importez votre Repository Gitlab. Dans le menu de gauche, allez dans Repositories et ajoutez votre repo Git cr\u00e9\u00e9 dans la journ\u00e9e. Attention de bien associer le Repository \u00e0 votre projet (qui porte le m\u00eame nom que votre Username). Un projet a \u00e9t\u00e9 cr\u00e9\u00e9 par Namespace et votre projet ne pourra d\u00e9ployer de ressources que dans ce Namespace. Cr\u00e9ez une application. Vous allez maintenant cr\u00e9er votre application. Dans ArgoCD, ajoutez une app avec le bouton appropri\u00e9 et le volet ci-dessous devrait s'afficher : Entrez le nom de l'application ${yourName}-cdb (les noms de projet sont uniques sous ArgoCD), choisissez bien votre projet , et ajustez la politique de synchronisation. Tip La politique de synchronisation d\u00e9finit si ArgoCD sera en synchronisation manuelle ou automatique. La synchronisation automatique, avec le Prune de Ressources et le Self-Healing sont vraiment les atouts recherch\u00e9s dans ArgoCD. ArgoCD aura donc pour objectif d'avoir un Cluster parfaitement synchronis\u00e9 avec Git et aura les pouvoirs de prendre toute action pour s'en approcher. Bien s\u00fbr, une gestion tr\u00e8s fine des droits de \"qu'est-ce que ArgoCD peut faire ou non\" est disponible dans le logiciel, mais ce n'est pas le sujet de ce TP. Choisissez votre Repository (il n'y a pas de restrictions de vue des diff\u00e9rents Repositories) et s\u00e9lectionnez la bonne branche ainsi que le chemin vers votre Chart (le dossier o\u00f9 se trouve le Chart.yaml et values.yaml , \u00e0 la racine du d\u00e9p\u00f4t . . Choisissez ensuite le Cluster (il n'y en a qu'un seul, le local), ainsi que le Namespace dans lequel vous allez d\u00e9ployer votre app. Le Namespace correspond \u00e0 votre username, vous ne pourrez pas d\u00e9ployer dans un autre Namespace. La derni\u00e8re partie s'affiche automatiquement car ArgoCD va examiner votre Repository. Comme vous avez utilis\u00e9 Helm, il suffit de s\u00e9lectionner le chemin du values.yaml (il doit \u00eatre propos\u00e9 directement). Normalement, c'est tout bon ! Il ne vous reste plus qu'\u00e0 valider. Pensez \u00e0 push sur votre r\u00e9pertoire git quand vous faites des changements, sinon ArgoCD ne pourra pas voir ces changements. Si tout s'est bien pass\u00e9, vous devriez voir avec fiert\u00e9 que votre app est renseign\u00e9e. Vous pouvez cliquer dessus, observer les diff\u00e9rentes ressources et leur \u00e9tat dans Kubernetes. F\u00e9licitations, votre application est maintenant en mode GitOps ! Question Pour v\u00e9rifier que tout fonctionne, essayez de d\u00e9truire un deployment manuellement dans votre Cluster. Que se passe-t-il ? Question Essayez de modifier le values.yaml en augmentant le replicaCount par exemple. Que se passe-t-il ? Bonus : un nouvel environnement L'utilisation d'ArgoCD n'\u00e9tait pas si complexe que \u00e7a. Sauf que pour l'instant, vous n'avez qu'un environnement, et Michel, votre Product Owner ador\u00e9, souhaiterait vraiment pouvoir disposer d'un environnement de pr\u00e9-production. De mani\u00e8re conventionnelle, les best-practices voudraient qu'on appr\u00e9hende cet environnement dans un autre Namespace et parfois m\u00eame dans un autre Cluster. Dans ce TP, on va s'autoriser \u00e0 en d\u00e9ployer un nouveau dans le m\u00eame Namespace. Cr\u00e9ez un nouveau fichier values appel\u00e9 values.staging.yaml , utilisez le nom de domaine staging. votreusername .takima.cloud, et cr\u00e9ez la ou les apps dans ArgoCD pour d\u00e9ployer la pr\u00e9-production de votre app. Pour faciliter la recherche, n'h\u00e9sitez pas \u00e0 cr\u00e9er un label staging sur toutes ces apps afin de faciliter l'affichage. \u00a9 Takima 2022","title":"Day 3 - GitOps"},{"location":"ch3-gitops/#tp-initiation-kubernetes-j3","text":"","title":"TP initiation Kubernetes (J3)"},{"location":"ch3-gitops/#objectifs","text":"Vous poss\u00e9dez maintenant toutes les ressources yaml qui d\u00e9crivent votre applicatif et vous pouvez donc d\u00e9ployer rapidement toute votre stack dans Kubernetes. C\u2019est un bon d\u00e9but ! Gr\u00e2ce \u00e0 cela, vous pouvez d\u00e9truire et construire un environnement complet facilement. De plus, toutes les probl\u00e9matiques d'exploitation sont g\u00e9r\u00e9es par Kubernetes (High Availability, Scaling, Auto Healing, Loadbalancing, etc.). Avant d'aller un peu plus loin dans la partie d\u00e9ploiement, un petit tour parmi le monde merveilleux des Operators Kubernetes s'impose !","title":"Objectifs"},{"location":"ch3-gitops/#initiation-aux-operators-kubernetes","text":"Les Operators Kubernetes permettent d'enrichir Kubernetes avec de nouvelles ressources personalis\u00e9es appel\u00e9es CRD (Custom Ressource Definition). Il en existe pour une multitude de produits et vous pouvez m\u00eame cr\u00e9er votre propre CRD si l'envie vous prend (si c'est utile bien s\u00fbr) ! Le cas de l'Operator Elasticsearch, appel\u00e9 ECK pour Elastic Cloud on Kubernetes , sera \u00e9tudi\u00e9 pour cette mise en pratique.","title":"Initiation aux Operators Kubernetes"},{"location":"ch3-gitops/#eck","text":"ECK est un Operator qui ajoute la possibilit\u00e9 de cr\u00e9er plusieurs CRDs, notamment : Elasticsearch Kibana Les noms sont assez explicites : Elasticsearch d\u00e9ploie un cluster Elasticsearch, et Kibana le dashboard de la stack ELK. Comme les Operators s'installent au niveau global du Cluster Kubernetes (un Operator ECK par Cluster), l'installation se fera en mode D\u00e9mo . Demo Voil\u00e0 l'Operator est install\u00e9, c'est maintenant \u00e0 vous de d\u00e9ployer une Stack Elastic !","title":"ECK"},{"location":"ch3-gitops/#la-ressource-elasticsearch","text":"Tout d'abord, cr\u00e9ez un dossier elk dans lequel vous \u00e9diterez les ressources YAML. Vous allez ensuite commencer par le plus important : la ressource elasticsearch . Editez un yaml elasticsearch.yaml . Quelqu'un s'est amus\u00e9 \u00e0 effacer des valeurs dans le yaml !! 1 2 3 4 5 6 7 8 9 10 11 apiVersion : elasticsearch.k8s.elastic.co/v1 kind : metadata : name : spec : version : 7.16.0 nodeSets : - name : default count : 1 config : node.store.allow_mmap : false 2. V\u00e9rifiez le bon d\u00e9ploiement d'Elasticsearch. #commandes utiles: kubectl get elasticsearch kubectl describe elasticsearch name 3. Observez quelles ressources natives ce CRD a cr\u00e9\u00e9.","title":"La ressource Elasticsearch"},{"location":"ch3-gitops/#la-ressource-kibana","text":"Que serait un cluster Elasticsearch sans son fid\u00e8le dashboard Kibana ? Cr\u00e9ez un fichier YAML kibana.yaml dans le dossier elk Par contre un petit bug est pass\u00e9 par l\u00e0, plusieurs lignes ont disparu !! Vous devez corriger ce qu'il manque dans elasticsearchRef 1 2 3 4 5 6 7 8 9 apiVersion : kibana.k8s.elastic.co/v1 kind : metadata : name : spec : version : count : 1 elasticsearchRef : name : 2. Cr\u00e9ez un Ingress pour pouvoir acc\u00e9der \u00e0 Kibana depuis le navigateur internet kibana-ingress.yaml . Pour l'Ingress, vous devriez y arriver avec tout ce que vous avez d\u00e9j\u00e0 appris. Tip Regarder les services que ES propose et les ports associ\u00e9s Attention le pod Kibana est en TLS il faut donc rajouter ces annotations dans votre ingress: nginx.ingress.kubernetes.io/backend-protocol: HTTPS Acc\u00e9dez au Kibana avec l'URL indiqu\u00e9e dans l'Ingress. Tip Pour l'authentification, observez le Secret -es-elastic-user qui contient le mot de passe et qui a \u00e9t\u00e9 cr\u00e9\u00e9 par le CRD Elasticsearch. L'utilisateur est elastic . Une fois connect\u00e9, Kibana vous propose de d\u00e9couvrir le produit, mais pas de temps \u00e0 perdre : cliquez directement sur Explore my own ! V\u00e9rifiez que Kibana est bien li\u00e9 au Cluster Elasticsearch. Pour se faire : Naviguez sur le menu en haut \u00e0 gauche. Cliquez sur Dev tools. Il est maintenant possible de requ\u00eater le cluster : Pour voir son \u00e9tat de sant\u00e9 et les nodes du cluster : GET _cat/health?v GET _cat/allocation?v Vous pouvez retrouver la taille du volume provisionn\u00e9.","title":"La ressource Kibana"},{"location":"ch3-gitops/#aller-plus-loin-avec-elk","text":"Scalez le Cluster ELK \u00e0 3 noeuds : \u00e9ditez le CRD Elasticsearch et observez les nouveaux Pods elasticsearch se cr\u00e9er et s'initialiser un par un. Retournez dans Kibana et relancer : GET _cat/health?v GET _cat/allocation?v Question Que constatez-vous ? V\u00e9rifiez que le Cluster fonctionne correctement. # Cr\u00e9\u00e9r l'index tp et ajouter un doc PUT tp/_doc/1 { \"body\": \"hello\" } # requ\u00eater le doc GET tp/_doc/1 # V\u00e9rifier que l'index tp existe bien et qu'il contient un seul doc GET _cat/indices?v Cet exemple assez simple permet de d\u00e9montrer qu'il est facile de cr\u00e9er des stacks Elasticsearch rapidement et \u00e0 la demande. Bien s\u00fbr, la configuration du CRD Elasticsearch ou Kibana peut aller bien plus loin (aussi loin que la config Elasticsearch classique le permet). Check Le dossier elk doit contenir les \u00e9l\u00e9ments suivants : . \u251c\u2500\u2500 elasticsearch.yaml \u251c\u2500\u2500 kibana.yaml \u2514\u2500\u2500 kibana-ingress.yaml","title":"Aller plus loin avec ELK"},{"location":"ch3-gitops/#helm","text":"Apr\u00e8s avoir fait un tour sur les Operator, retour aux ressources pr\u00e9c\u00e9dentes : l'API, le Front et la base de donn\u00e9es. Vous vous demandez peut \u00eatre comment g\u00e9rer plusieurs environnements, des variables entre les ressources YAML, des conditions en fonction de l'environnement, etc. Aujourd'hui ce n'est pas possible, n'est-ce-pas ? Surtout que dans cette formation vous utilisez un petit applicatif mais imaginez que vous en ayez des centaines \u00e0 g\u00e9rer (avec les architectures microservices, \u00e7a peut arriver vite !) : cela deviendrait rapidement ing\u00e9rable. Le constat est l\u00e0 : les ressources actuelles sont assez statiques en l\u2019\u00e9tat. Il est possible d'industrialiser tout cela.","title":"Helm"},{"location":"ch3-gitops/#installation","text":"Dans un premier temps, vous allez installer Helm, ArgoCD & Git.","title":"Installation"},{"location":"ch3-gitops/#linux-package-manager-debianubuntu","text":"curl https://baltocdn.com/helm/signing.asc | sudo apt-key add - sudo apt-get install apt-transport-https --yes echo \"deb https://baltocdn.com/helm/stable/debian/ all main\" | sudo tee /etc/apt/sources.list.d/helm-stable-debian.list sudo apt-get update sudo apt-get install helm # git add-apt-repository ppa:git-core/ppa apt update apt install git","title":"Linux - Package manager - Debian/Ubuntu"},{"location":"ch3-gitops/#linux-script","text":"curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 chmod 700 get_helm.sh ./get_helm.sh # must works fine helm version # git # utiliser un gestionnaire de packet ou un t\u00e9l\u00e9chargement direct : https://git-scm.com/book/fr/v2/D%C3%A9marrage-rapide-Installation-de-Git","title":"Linux - Script"},{"location":"ch3-gitops/#macos","text":"brew install helm # must works fine helm version # Git https://git-scm.com/download/mac","title":"MacOS"},{"location":"ch3-gitops/#windows","text":"# Helm https://helm.sh/docs/intro/install/ # ArgoCD https://github.com/argoproj/argo-cd/releases/download/v2.2.3/argocd-windows-amd64.exe # Git http://git-scm.com/download/win","title":"Windows"},{"location":"ch3-gitops/#faire-des-templates-avec-helm","text":"","title":"Faire des templates avec Helm"},{"location":"ch3-gitops/#preambule","text":"Helm est un outil indispensable pour d\u00e9ployer des stacks applicatives compl\u00e8tes dans Kubernetes. Pratique, c\u2019est un projet maintenu par la CNCF . L\u2019id\u00e9e est simple : permettre de faire des templates de toutes les ressources yaml d\u2019une application donn\u00e9e et les d\u00e9ployer sur Kubernetes ! En d\u00e9finitive, cela pourrait s'apparenter \u00e0 la possibli\u00e9 de d\u00e9ployer des plateformes compl\u00e8tes, clef en main, un peu sur le mod\u00e8le PaaS, Plateforme as a Service. Une stack applicative qui se d\u00e9ploie via Helm se nomme un Chart Helm . L\u2019objectif ici est donc de cr\u00e9er un Chart Helm pour votre stack ComputerDatabase.","title":"Pr\u00e9ambule"},{"location":"ch3-gitops/#setup","text":"Avant d'utiliser Helm, vous allez mettre en place un d\u00e9p\u00f4t Git avec les ressources Kubernetes fournises pour ce TP. Cela permettra plus tard d'y connecter ArgoCD. Vous pouvez passer directement \u00e0 l'\u00e9tape 3 si vous avez d\u00e9j\u00e0 un compte Git configur\u00e9 sur Gitlab.com. Cr\u00e9er un compte sur https://gitlab.com/ . Configurez un mot de passe \u00e0 votre compte si vous vous \u00eates connect\u00e9 sans \u00e9diter de mot de passe page suivante . Cr\u00e9ez un repository sur Gitlab https://gitlab.com/projects/new#blank_project . Commandes \u00e0 lancer une fois votre repository cr\u00e9\u00e9 : git clone https://gitlab.com/votreUser/ch3_gitops.git # votre username & password de gitlab seront demand\u00e9s. cd ch3_gitops T\u00e9l\u00e9chargez les premi\u00e8res ressources pour Helm. D\u00e9compressez le contenu de l'archive et d\u00e9placez le contenu du dossier k8s-trainees-main/boilerplate/day-3/step-1 dans votre repository Git ch3_gitops . Une fois les ressources t\u00e9l\u00e9charg\u00e9es, faites un premier commit et push ! git commit -am \"Init Helm resources\" git push origin main Une fois l'\u00e9tape de commit r\u00e9alis\u00e9e vous pouvez jeter un coup d'oeil aux diff\u00e9rents \u00e9l\u00e9ments fournis : templates/ : contient une version template de toutes les ressources Kubernetes que vous devez d\u00e9ployer. Chart.yaml : d\u00e9crit votre chart (la version, le nom, les mainteners). values.yaml : contient toutes les variables qui seront utilis\u00e9es dans les ressources. D\u2019une certaine mani\u00e8re, il d\u00e9crit de mani\u00e8re centrale l'infra \u00e0 d\u00e9ployer. Regardez d'un peu plus pr\u00e8s un Template, le Service du Front : templates/front-service.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 {{ - if .Values.front.enabled }} apiVersion : v1 kind : Service metadata : name : {{ .Values.front.serviceName }} spec : selector : app : front ports : - protocol : TCP port : {{ .Values.front.servicePort }} targetPort : 80 {{ end }} L'usage de variables Helm est soulign\u00e9 sur les deux lignes en jaune (lignes 5 & 11). Chaque variable fait r\u00e9f\u00e9rence \u00e0 une valeur dans le fichier values.yaml : values.yaml 22 23 24 25 26 27 28 29 30 ... front : enabled : true image : repository : master3.takima.io:4567/master3/kubernetes-resources/front tag : latest serviceName : 'front' servicePort : 80 ... Info Il est donc possible de facilement variabiliser des \u00e9l\u00e9ments en pla\u00e7ant les valeurs dans les fichiers values.yaml et y faisant r\u00e9f\u00e9rence via la syntaxe {{ .Values.x.y }} . Info Par la m\u00eame occasion, vous remarquerez que la premi\u00e8re ligne est une condition sur la variable .Values.front.enabled qui englobe l'ensemble du fichier. Ce qui se traduit par : si le Front est enable tout le fichier sera g\u00e9n\u00e9r\u00e9 et cette ressource sera donc d\u00e9ploy\u00e9e. templates/front-service.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 {{ - if .Values.front.enabled }} apiVersion : v1 kind : Service metadata : name : {{ .Values.front.serviceName }} spec : selector : app : front ports : - protocol : TCP port : {{ .Values.front.servicePort }} targetPort : 80 {{ end }} Vous pouvez ensuite tester Helm pour g\u00e9n\u00e9rer vos ressources \u00e0 partir de vos Templates et Ressources : helm template --values ./values.yaml ./ --output-dir dist Vous retrouverez les ressources Kubernetes g\u00e9n\u00e9r\u00e9es avec vos valeurs dans le dossier dist ! Tip Vous pouvez aussi utiliser la commande suivante pour debug votre Chart sans cr\u00e9er de fichiers : helm install cdb --dry-run --debug ./ Vous pouvez tester de d\u00e9ployer l'ensemble de votre Chart sur votre cluster avec la commande suivante : helm install cdb ./ Info La commande helm install|upgrade n'utilse pas les fichiers g\u00e9n\u00e9r\u00e9s dans le dossier dist , elle utilise directement vos sources. Help Vous aurez certainement besoin de configurer le Secret (avec les identifiants re\u00e7us par mail lors du premier jour) pour pull les images : kubectl create secret docker-registry auth-master3-registry --docker-server = master3.takima.io:4567 --docker-username = <USERNAME> --docker-password = <PASSWORD>","title":"Setup"},{"location":"ch3-gitops/#a-vous-de-jouer","text":"Maintenant que votre \u00e9quipe vous a fourni le Front , \u00e0 vous de faire la m\u00eame chose pour l' API et la DB ! Info Rappel des ressources \u00e0 g\u00e9n\u00e9rer : 1. api-deployement.yaml 2. api-ingress.yaml 3. api-service.yaml 4. api-config.yaml 5. pg-credentials.yaml 6. pg-deployment.yaml 7. pg-pvc.yaml 8. pg-service.yaml Help Vous pouvez reprendre les ressources cr\u00e9\u00e9es dans la partie pr\u00e9c\u00e9dente pour vous aider : https://gitlab.takima.io/school/k8s-trainees/-/archive/main/k8s-trainees-main.tar.gz?path=boilerplate/day-2/step-5 Tip Pensez \u00e0 bien utiliser le syst\u00e8me de templating avec le fichier values.yaml et les diff\u00e9rents outils de Helm . Une fois termin\u00e9, vous pouvez mettre \u00e0 jour votre Chart sur le Cluster avec la commande : helm upgrade cdb ./","title":"\u00c0 vous de jouer !"},{"location":"ch3-gitops/#bonus-1","text":"Et si je vous demandais de changer le nom de votre application ? Oui, quand on fait du templating, on remarque aussi que parfois, il y a beaucoup de \"redite\". Les labels, le nom de l'application, etc... C'est pour \u00e7a que helm vous permet de cr\u00e9er des variables suppl\u00e9mentaires. Par convention, nous recommandons de cr\u00e9er un fichier templates/_helpers.tpl . De mani\u00e8re traditionnelle, le fichier peut contenir quelques lignes comme \u00e7a : {{ /* Expand the name of the chart. */ }} {{ - define \"MyAppCtx.name\" - }} {{ - default .Chart.Name | trunc 63 | trimSuffix \"-\" }} {{ - end }} {{ /* Application image tag We select by default the Chart appVersion or an override in values */ }} {{ - define \"MyAppCtx.imageTag\" }} {{ - $name : = default .Chart.AppVersion .Values.image.tag }} {{ - printf \"%s\" $name }} {{ - end }} {{ /* Create a default fully qualified app name. We truncate at 63 chars because some Kubernetes name fields are limited to this ( by the DNS naming spec ) . */ }} {{ - define \"MyAppCtx.fullname\" }} {{ - $name : = default .Chart.Name .Values.nameOverride - }} {{ - printf \"%s-%s\" .Release.Name $name | trunc 63 | trimSuffix \"-\" }} {{ - end }} {{ /* Create chart name and version as used by the chart label. */ }} {{ - define \"MyAppCtx.chart\" - }} {{ - printf \"%s-%s\" .Chart.Name .Chart.Version | replace \"+\" \"_\" | trunc 63 | trimSuffix \"-\" }} {{ - end }} {{ /* Common labels */ }} {{ - define \"MyAppCtx.labels\" - }} helm.sh/chart: {{ include \"MyAppCtx.chart\" . }} {{ include \"MyAppCtx.selectorLabels\" . }} {{ - if .Chart.AppVersion }} app.kubernetes.io/version: {{ .Chart.AppVersion | quote }} {{ - end }} app.kubernetes.io/managed-by: {{ .Release.Service }} {{ - end }} {{ /* Selector labels */ }} {{ - define \"MyAppCtx.selectorLabels\" - }} app.kubernetes.io/name: {{ include \"MyAppCtx.name\" . }} app.kubernetes.io/instance: {{ .Release.Name }} {{ - end }} Ces variables pourront \u00eatre utilis\u00e9es dans votre chart. Appliquez les variables qui vous semblent importantes dans l'ensemble de vos ressources. Objectifs : * une modification du chart name ou du nameOverride dans les values doit modifier le nom de toutes vos ressources. * vous ne devriez plus avoir besoin de serviceName ou tls secretName dans vos values.yaml. Faites bien attention \u00e0 vos selectors et matchSelectors !","title":"Bonus 1"},{"location":"ch3-gitops/#bonus-2","text":"Peut-\u00eatre avez-vous remarqu\u00e9 qu'une modification de ConfigMap ne red\u00e9marrait pas les deployments. C'est normal, car vos applications sont cens\u00e9es pouvoir consommer les ConfigMaps d\u00e8s qu'elles le souhaitent. Pour forcer un red\u00e9ploiement lors de la modification d'une ConfigMap associ\u00e9e, vous pouvez d\u00e9clarer une annotation suppl\u00e9mentaire dans le Deployment concern\u00e9 : annotations : - checksum/config : {{ include (print $.Template.BasePath \"/configmap.yaml\") . | sha256sum }} D\u00e8s que le hash de la configmap changera, le deployment sera consid\u00e9r\u00e9 comme diff\u00e9rent, et donc appliqu\u00e9.","title":"Bonus 2"},{"location":"ch3-gitops/#bonus-2_1","text":"Maintenant, votre \u00e9quipe revient vers vous et souhaite ajouter un nouvel environnement de test. Aucun probl\u00e8me car vous avez d\u00e9j\u00e0 toute la structure avec Helm template ! L'environnement de test se pr\u00e9sente comme suit : API_URL = api.staging.votreusername.takima.cloud WWW_URL = www.staging.votreusername.takima.cloud Tip Vous pouvez ajouter autant de Values que vous le souhaitez avec la commande Helm : helm template --values ./values.yaml --values ./values.env.yaml ./ --output-dir dist","title":"Bonus 2"},{"location":"ch3-gitops/#a-vous-de-jouer_1","text":"Avec toutes les informations dont vous diposez, vous pouvez cr\u00e9er les fichiers n\u00e9cessaires pour que votre Chart s'adapte facilement avec le nouvel environnement.","title":"\u00c0 vous de jouer"},{"location":"ch3-gitops/#gitops-avec-argocd","text":"Avant de commencer cette partie, pensez \u00e0 d\u00e9truire l'environnement cr\u00e9\u00e9 avec vos templates Helm : helm delete mon-app Vous savez faire un joli Template Helm que vous d\u00e9ployez \u00e0 la main. La premi\u00e8re chose \u00e0 dire, c'est qu'il existe de multiples mani\u00e8res de faire du GitOps : il serait facile de mettre en oeuvre un peu de scripting avec Gitlab-CI par exemple, pour faire un helm upgrade... automatiquement lorsqu'on veut d\u00e9ployer un applicatif sur un cluster. L'inconv\u00e9nient avec cette approche est que le flux serait uniquement descendant (de Gitlab au Cluster). Argo CD a deux r\u00f4les majeurs : Faciliter la configuration de ces jobs (plus facile que via du scripting). Synchroniser en permanence l'\u00e9tat du Cluster avec celui de Git, m\u00eame si les ressources du Cluster changent de leur c\u00f4t\u00e9. Vous aurez l'occasion de v\u00e9rifier et constater cela dans le TP !","title":"GitOps avec ArgoCD"},{"location":"ch3-gitops/#accedez-au-portail-argocd","text":"Rendez-vous sur https://argocd.takima.cloud et entrez votre username / mot de passe (cf mail envoy\u00e9).","title":"Acc\u00e9dez au portail ArgoCD."},{"location":"ch3-gitops/#importez-votre-repository-gitlab","text":"Dans le menu de gauche, allez dans Repositories et ajoutez votre repo Git cr\u00e9\u00e9 dans la journ\u00e9e. Attention de bien associer le Repository \u00e0 votre projet (qui porte le m\u00eame nom que votre Username). Un projet a \u00e9t\u00e9 cr\u00e9\u00e9 par Namespace et votre projet ne pourra d\u00e9ployer de ressources que dans ce Namespace.","title":"Importez votre Repository Gitlab."},{"location":"ch3-gitops/#creez-une-application","text":"Vous allez maintenant cr\u00e9er votre application. Dans ArgoCD, ajoutez une app avec le bouton appropri\u00e9 et le volet ci-dessous devrait s'afficher : Entrez le nom de l'application ${yourName}-cdb (les noms de projet sont uniques sous ArgoCD), choisissez bien votre projet , et ajustez la politique de synchronisation. Tip La politique de synchronisation d\u00e9finit si ArgoCD sera en synchronisation manuelle ou automatique. La synchronisation automatique, avec le Prune de Ressources et le Self-Healing sont vraiment les atouts recherch\u00e9s dans ArgoCD. ArgoCD aura donc pour objectif d'avoir un Cluster parfaitement synchronis\u00e9 avec Git et aura les pouvoirs de prendre toute action pour s'en approcher. Bien s\u00fbr, une gestion tr\u00e8s fine des droits de \"qu'est-ce que ArgoCD peut faire ou non\" est disponible dans le logiciel, mais ce n'est pas le sujet de ce TP. Choisissez votre Repository (il n'y a pas de restrictions de vue des diff\u00e9rents Repositories) et s\u00e9lectionnez la bonne branche ainsi que le chemin vers votre Chart (le dossier o\u00f9 se trouve le Chart.yaml et values.yaml , \u00e0 la racine du d\u00e9p\u00f4t . . Choisissez ensuite le Cluster (il n'y en a qu'un seul, le local), ainsi que le Namespace dans lequel vous allez d\u00e9ployer votre app. Le Namespace correspond \u00e0 votre username, vous ne pourrez pas d\u00e9ployer dans un autre Namespace. La derni\u00e8re partie s'affiche automatiquement car ArgoCD va examiner votre Repository. Comme vous avez utilis\u00e9 Helm, il suffit de s\u00e9lectionner le chemin du values.yaml (il doit \u00eatre propos\u00e9 directement). Normalement, c'est tout bon ! Il ne vous reste plus qu'\u00e0 valider. Pensez \u00e0 push sur votre r\u00e9pertoire git quand vous faites des changements, sinon ArgoCD ne pourra pas voir ces changements. Si tout s'est bien pass\u00e9, vous devriez voir avec fiert\u00e9 que votre app est renseign\u00e9e. Vous pouvez cliquer dessus, observer les diff\u00e9rentes ressources et leur \u00e9tat dans Kubernetes. F\u00e9licitations, votre application est maintenant en mode GitOps ! Question Pour v\u00e9rifier que tout fonctionne, essayez de d\u00e9truire un deployment manuellement dans votre Cluster. Que se passe-t-il ? Question Essayez de modifier le values.yaml en augmentant le replicaCount par exemple. Que se passe-t-il ?","title":"Cr\u00e9ez une application."},{"location":"ch3-gitops/#bonus-un-nouvel-environnement","text":"L'utilisation d'ArgoCD n'\u00e9tait pas si complexe que \u00e7a. Sauf que pour l'instant, vous n'avez qu'un environnement, et Michel, votre Product Owner ador\u00e9, souhaiterait vraiment pouvoir disposer d'un environnement de pr\u00e9-production. De mani\u00e8re conventionnelle, les best-practices voudraient qu'on appr\u00e9hende cet environnement dans un autre Namespace et parfois m\u00eame dans un autre Cluster. Dans ce TP, on va s'autoriser \u00e0 en d\u00e9ployer un nouveau dans le m\u00eame Namespace. Cr\u00e9ez un nouveau fichier values appel\u00e9 values.staging.yaml , utilisez le nom de domaine staging. votreusername .takima.cloud, et cr\u00e9ez la ou les apps dans ArgoCD pour d\u00e9ployer la pr\u00e9-production de votre app. Pour faciliter la recherche, n'h\u00e9sitez pas \u00e0 cr\u00e9er un label staging sur toutes ces apps afin de faciliter l'affichage. \u00a9 Takima 2022","title":"Bonus : un nouvel environnement"},{"location":"cheatsheet/","text":"Cheatsheet Cr\u00e9ation d'une ressource \u00e0 partir d'un fichier YAML : kubectl apply -f FICHIER.yaml Suppression d'une ressource Kubernetes : # RESSOURCE est le type de ressource Kubernetes, valeurs possibles : pod, service, deploy, ingress, secret, namespace, etc # NOM_DE_LA_RESSOURCE est la nom de votre ressource dans votre namespace kubectl delete RESSOURCE NOM_DE_LA_RESSOURCE Rentrer dans un pod : # Ici on rentre en mode interactif avec la commande /bin/sh dans NOM_DU_POD kubectl exec -it NOM_DU_POD -- /bin/sh Avoir le descriptif d'une ressource d\u00e9ploy\u00e9e sous forme de YAML : # RESSOURCE est le type de ressource Kubernetes, valeurs possibles : pod, service, deploy, ingress, secret, namespace, etc # NOM_DE_LA_RESSOURCE est la nom de votre ressource dans votre namespace kubectl get RESSOURCE NOM_DE_LA_RESSOURCE -o = yaml D\u00e9crire la ressource d\u00e9ploy\u00e9e (utile pour d\u00e9bugger car permet de voir les diff\u00e9rents \u00e9v\u00e8nements apr\u00e8s le d\u00e9ploiement) # RESSOURCE est le type de ressource Kubernetes, valeurs possibles : pod, service, deploy, ingress, secret, namespace, etc # NOM_DE_LA_RESSOURCE est la nom de votre ressource dans votre namespace kubectl describe RESSOURCE NOM_DE_LA_RESSOURCE Cr\u00e9ation du Secret pour Docker registry : # PASSWORD a \u00e9t\u00e9 envoy\u00e9 le premier jour par email. # USERNAME est la votre premi\u00e8re lettre de votre pr\u00e9nom suivi de votre nom. kubectl create secret docker-registry auth-master3-registry --docker-server = master3.takima.io:4567 --docker-username = readregcred --docker-password = PASSWORD -n USERNAME Avoir les logs d'un pod : # -f pour follow kubectl logs NOM_DU_POD -f","title":"Cheatsheet"},{"location":"cheatsheet/#cheatsheet","text":"Cr\u00e9ation d'une ressource \u00e0 partir d'un fichier YAML : kubectl apply -f FICHIER.yaml Suppression d'une ressource Kubernetes : # RESSOURCE est le type de ressource Kubernetes, valeurs possibles : pod, service, deploy, ingress, secret, namespace, etc # NOM_DE_LA_RESSOURCE est la nom de votre ressource dans votre namespace kubectl delete RESSOURCE NOM_DE_LA_RESSOURCE Rentrer dans un pod : # Ici on rentre en mode interactif avec la commande /bin/sh dans NOM_DU_POD kubectl exec -it NOM_DU_POD -- /bin/sh Avoir le descriptif d'une ressource d\u00e9ploy\u00e9e sous forme de YAML : # RESSOURCE est le type de ressource Kubernetes, valeurs possibles : pod, service, deploy, ingress, secret, namespace, etc # NOM_DE_LA_RESSOURCE est la nom de votre ressource dans votre namespace kubectl get RESSOURCE NOM_DE_LA_RESSOURCE -o = yaml D\u00e9crire la ressource d\u00e9ploy\u00e9e (utile pour d\u00e9bugger car permet de voir les diff\u00e9rents \u00e9v\u00e8nements apr\u00e8s le d\u00e9ploiement) # RESSOURCE est le type de ressource Kubernetes, valeurs possibles : pod, service, deploy, ingress, secret, namespace, etc # NOM_DE_LA_RESSOURCE est la nom de votre ressource dans votre namespace kubectl describe RESSOURCE NOM_DE_LA_RESSOURCE Cr\u00e9ation du Secret pour Docker registry : # PASSWORD a \u00e9t\u00e9 envoy\u00e9 le premier jour par email. # USERNAME est la votre premi\u00e8re lettre de votre pr\u00e9nom suivi de votre nom. kubectl create secret docker-registry auth-master3-registry --docker-server = master3.takima.io:4567 --docker-username = readregcred --docker-password = PASSWORD -n USERNAME Avoir les logs d'un pod : # -f pour follow kubectl logs NOM_DU_POD -f","title":"Cheatsheet"}]}